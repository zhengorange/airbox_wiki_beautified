{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"benchmarks.html","title":"Benchmarks","text":""},{"location":"benchmarks.html#llm","title":"LLM","text":"<p>For running LLM benchmarks, see the <code>MLC</code> container documentation.</p>"},{"location":"benchmarks.html#vlm","title":"VLM","text":"<p>For running VLM benchmarks, see the <code>MLC</code> and <code>MiniGPT-4</code> container documentation.</p>"},{"location":"benchmarks.html#vit","title":"ViT","text":"<p>VIT performance data from [1] [2] [3]</p>"},{"location":"benchmarks.html#stable-diffusion","title":"Stable Diffusion","text":""},{"location":"benchmarks.html#riva","title":"Riva","text":"<p>For running Riva benchmarks, see ASR Performance and TTS Performance.</p>"},{"location":"benchmarks.html#vector-database","title":"Vector Database","text":"<p>For running vector database benchmarks, see the <code>NanoDB</code> container documentation.</p>"},{"location":"community_articles.html","title":"Community Articles","text":"<p>Below, you'll find a collection of community articles showcasing the implementation of generative AI on the Jetson platform.</p>"},{"location":"community_articles.html#nvidia-bringing-generative-ai-to-life-with-nvidia-jetson-10-19-2023","title":"NVIDIA Bringing Generative AI to Life with NVIDIA Jetson (10-19-2023)","text":"<p>Follow this walkthrough of the Jetson AI Lab tutorials along with coverage of the latest features and advances coming to JetPack 6 and beyond:</p> <p> https://developer.nvidia.com/blog/bringing-generative-ai-to-life-with-jetson/</p>"},{"location":"community_articles.html#medium-how-to-set-up-your-jetson-device-for-llm-inference-and-fine-tuning-10-02-2023","title":"Medium How to set up your Jetson device for LLM inference and fine-tuning (10-02-2023)","text":"<p>Michael Yuan's guide demonstrating how to set up the Jetson AGX Orin 64GB Developer Kit specifically for large language model (LLM) inference, highlighting the crucial role of GPUs and the cost-effectiveness of the Jetson AGX Orin for LLM tasks.</p> <p> https://medium.com/@michaelyuan_88928/how-to-set-up-your-jetson-device-for-llm-inference-and-fine-tuning-682e36444d43</p>"},{"location":"community_articles.html#hackster-getting-started-with-ai-on-nvidia-jetson-agx-orin-dev-kit-09-16-2023","title":"Hackster Getting Started with AI on Nvidia Jetson AGX Orin Dev Kit (09-16-2023)","text":"<p>Nurgaliyev Shakhizat demonstrates <code>llamaspeak</code> on Jetson AGX Orin 64GB Developer Kit.</p> <p></p> <p> https://www.hackster.io/shahizat/getting-started-with-ai-on-nvidia-jetson-agx-orin-dev-kit-5a55b5</p>"},{"location":"community_articles.html#hackster-new-ai-tool-is-generating-a-lot-of-buzz-09-13-2023","title":"Hackster New AI Tool Is Generating a Lot of Buzz (09-13-2023)","text":"<p>Nick Bild provides an insightful introduction to the Jetson Generative AI Playground.</p> <p> https://www.hackster.io/news/new-ai-tool-is-generating-a-lot-of-buzz-3cc5f23a3598</p>"},{"location":"community_articles.html#jetsonhacks-use-these-jetson-docker-containers-tutorial-09-04-2023","title":"JetsonHacks Use These! Jetson Docker Containers Tutorial (09-04-2023)","text":"<p>JetsonHacks has a in-depth tutorial on how to use <code>jetson-containers</code> and even show <code>text-generation-webui</code> and <code>stable-diffusion-webui</code> containers in action!</p> <p> https://jetsonhacks.com/2023/09/04/use-these-jetson-docker-containers-tutorial/</p>"},{"location":"community_articles.html#hackster-llama-2-llms-w-nvidia-jetson-and-textgeneration-web-ui-08-17-2023","title":"Hackster LLaMa 2 LLMs w/ NVIDIA Jetson and textgeneration-web-ui (08-17-2023)","text":"<p>Paul DeCarlo demonstrates 13B and 70B parameter LLama 2 models running locally on Jetson AGX Orin 64GB Developer Kit.</p> <p></p> <p> https://www.hackster.io/pjdecarlo/llama-2-llms-w-nvidia-jetson-and-textgeneration-web-ui-96b070</p>"},{"location":"community_articles.html#hackster-running-a-chatgpt-like-llm-llama2-on-a-nvidia-jetson-cluster-08-14-2023","title":"Hackster Running a ChatGPT-Like LLM-LLaMA2 on a Nvidia Jetson Cluster (08-14-2023)","text":"<p>Discover how to run a LLaMA-2 7B model on an NVIDIA Jetson cluster in this insightful tutorial by Nurgaliyev Shakhizat.</p> <p></p> <p> https://www.hackster.io/shahizat/running-a-chatgpt-like-llm-llama2-on-a-nvidia-jetson-cluster-cbc7d4</p>"},{"location":"community_articles.html#jetsonhacks-speech-ai-on-nvidia-jetson-tutorial-08-07-2023","title":"JetsonHacks Speech AI on NVIDIA Jetson Tutorial (08-07-2023)","text":"<p>JetsonHacks gives a nice introduction to NVIDIA RIVA SDK and demonstrate its automated speech recognition (ASR) capability on Jetson Orin Nano Developer Kit.</p> <p> https://jetsonhacks.com/2023/08/07/speech-ai-on-nvidia-jetson-tutorial/</p>"},{"location":"community_articles.html#hackster-llm-based-multimodal-ai-w-azure-open-ai-nvidia-jetson-07-12-2023","title":"Hackster LLM based Multimodal AI w/ Azure Open AI &amp; NVIDIA Jetson (07-12-2023)","text":"<p>Learn how to harness the power of Multimodal AI by running Microsoft JARVIS on an Jetson AGX Orin 64GB Developer Kit, enabling a wide range of AI tasks with ChatGPT-like capabilities, image generation, and more, in this comprehensive guide by Paul DeCarlo.</p> <p></p> <p> https://www.hackster.io/pjdecarlo/llm-based-multimodal-ai-w-azure-open-ai-nvidia-jetson-135ff2</p>"},{"location":"community_articles.html#hackster-how-to-run-a-chatgpt-like-llm-on-nvidia-jetson-board-06-13-2023","title":"Hackster How to Run a ChatGPT-Like LLM on NVIDIA Jetson board (06-13-2023)","text":"<p>Nurgaliyev Shakhizat explores voice AI assistant on Jetson using FastChat and VoskAPI.</p> <p></p> <p> https://www.hackster.io/shahizat/how-to-run-a-chatgpt-like-llm-on-nvidia-jetson-board-41fd79</p>"},{"location":"getting-started.html","title":"Getting started","text":""},{"location":"tips_ram-optimization.html","title":"RAM Optimization","text":"<p>Running a LLM requires a huge RAM space.</p> <p>Especially if you are on Jetson Orin Nano that only has 8GB of RAM, it is crucial to leave as much RAM space available for models. </p> <p>Here we share a couple of ways to optimize the system RAM usage. </p>"},{"location":"tips_ram-optimization.html#disabling-the-desktop-gui","title":"Disabling the Desktop GUI","text":"<p>If you use your Jetson remotely through SSH, you can disable the Ubuntu desktop GUI. This will free up extra memory that the window manager and desktop uses (around ~800MB for Unity/GNOME).</p> <p>You can disable the desktop temporarily, run commands in the console, and then re-start the desktop when desired:</p> <pre><code>$ sudo init 3     # stop the desktop\n# log your user back into the console (Ctrl+Alt+F1, F2, ect)\n$ sudo init 5     # restart the desktop\n</code></pre> <p>If you wish to make this persistent across reboots, you can use the following commands to change the boot-up behavior:</p> <ul> <li> <p>To disable desktop on boot</p> <pre><code>sudo systemctl set-default multi-user.target\n</code></pre> </li> <li> <p>To enable desktop on boot</p> <pre><code>sudo systemctl set-default graphical.target\n</code></pre> </li> </ul>"},{"location":"tips_ram-optimization.html#disabling-misc-services","title":"Disabling misc services","text":"<pre><code>sudo systemctl disable nvargus-daemon.service\n</code></pre>"},{"location":"tips_ram-optimization.html#mounting-swap","title":"Mounting Swap","text":"<p>If you're building containers or working with large models, it's advisable to mount SWAP (typically correlated with the amount of memory in the board). Run these commands to disable ZRAM and create a swap file:</p> <p>If you have NVMe SSD storage available, it's preferred to allocate the swap file on the NVMe SSD.</p> <pre><code>sudo systemctl disable nvzramconfig\nsudo fallocate -l 16G /ssd/16GB.swap\nsudo mkswap /ssd/16GB.swap\nsudo swapon /ssd/16GB.swap\n</code></pre> <p>Then add the following line to the end of /etc/fstab to make the change persistent:</p> <pre><code>/ssd/16GB.swap  none  swap  sw 0  0\n</code></pre>"},{"location":"tips_ssd-docker.html","title":"Tips - SSD + Docker","text":"<p>Once you have your Jetson set up by flashing the latest Jetson Linux (L4T) BSP on it or by flashing the SD card with the whole JetPack image, before embarking on testing out all the great generative AI application using <code>jetson-containers</code>, you want to make sure you have a huge storage space for all the containers and the models you will download.  </p> <p>We are going to show how you can install SSD on your Jetson, and set it up for Docker.</p>"},{"location":"tips_ssd-docker.html#ssd","title":"SSD","text":""},{"location":"tips_ssd-docker.html#physical-installation","title":"Physical installation","text":"<ol> <li>Unplug power and any peripherals from the Jetson developer kit.</li> <li>Physically install an NVMe SSD card on the carrier board of your Jetson developer kit, making sure to properly seat the connector and secure with the screw.</li> <li>Reconnect any peripherals, and then reconnect the power supply to turn on the Jetson developer kit.</li> <li> <p>Once the system is up, verify that your Jetson identifies a new memory controller on PCI bus:</p> <pre><code>lspci\n</code></pre> <p>The output should look like the following:</p> <pre><code>0007:01:00.0 Non-Volatile memory controller: Marvell Technology Group Ltd. Device 1322 (rev 02)\n</code></pre> </li> </ol>"},{"location":"tips_ssd-docker.html#format-and-set-up-auto-mount","title":"Format and set up auto-mount","text":"<ol> <li> <p>Run <code>lsblk</code> to find the device name.</p> <pre><code>lsblk\n</code></pre> <p>The output should look like the following:</p> <pre><code>NAME         MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\nloop0          7:0    0    16M  1 loop \nmmcblk1      179:0    0  59.5G  0 disk \n\u251c\u2500mmcblk1p1  179:1    0    58G  0 part /\n\u251c\u2500mmcblk1p2  179:2    0   128M  0 part \n\u251c\u2500mmcblk1p3  179:3    0   768K  0 part \n\u251c\u2500mmcblk1p4  179:4    0  31.6M  0 part \n\u251c\u2500mmcblk1p5  179:5    0   128M  0 part \n\u251c\u2500mmcblk1p6  179:6    0   768K  0 part \n\u251c\u2500mmcblk1p7  179:7    0  31.6M  0 part \n\u251c\u2500mmcblk1p8  179:8    0    80M  0 part \n\u251c\u2500mmcblk1p9  179:9    0   512K  0 part \n\u251c\u2500mmcblk1p10 179:10   0    64M  0 part \n\u251c\u2500mmcblk1p11 179:11   0    80M  0 part \n\u251c\u2500mmcblk1p12 179:12   0   512K  0 part \n\u251c\u2500mmcblk1p13 179:13   0    64M  0 part \n\u2514\u2500mmcblk1p14 179:14   0 879.5M  0 part \nzram0        251:0    0   1.8G  0 disk [SWAP]\nzram1        251:1    0   1.8G  0 disk [SWAP]\nzram2        251:2    0   1.8G  0 disk [SWAP]\nzram3        251:3    0   1.8G  0 disk [SWAP]\nnvme0n1      259:0    0 238.5G  0 disk \n</code></pre> <p>Identify the device corresponding to your SSD. In this case, it is <code>nvme0n1</code>.</p> </li> <li> <p>Format the SSD, create a mount point, and mount it to the filesystem.</p> <pre><code>sudo mkfs.ext4 /dev/nvme0n1\n</code></pre> <p>You can choose any name for the mount point directory. We use <code>/ssd</code> here, but in <code>jetson-containers</code>' setup.md documentation, <code>/mnt</code> is used.  </p> <pre><code>sudo mkdir /ssd\n</code></pre> <pre><code>sudo mount /dev/nvme0n1 /ssd\n</code></pre> </li> <li> <p>In order to ensure that the mount persists after boot, add an entry to the <code>fstab</code> file:</p> <p>First, identify the UUID for your SSD:</p> <pre><code>lsblk -f\n</code></pre> <p>Then, add a new entry to the <code>fstab</code> file:</p> <pre><code>sudo vi /etc/fstab\n</code></pre> <p>Insert the following line, replacing the UUID with the value found from <code>lsblk -f</code>:</p> <pre><code>UUID=************-****-****-****-******** /ssd/ ext4 defaults 0 2\n</code></pre> </li> <li> <p>Finally, change the ownership of the <code>/ssd</code> directory.</p> <pre><code>sudo chown ${USER}:${USER} /ssd\n</code></pre> </li> </ol>"},{"location":"tips_ssd-docker.html#docker","title":"Docker","text":"<ol> <li> <p>Install the full NVIDIA JetPack SDK, which includes the <code>nvidia-container</code> package.</p> <p>Note: If you used an NVIDIA-supplied SD card image to flash your SD card, all necessary JetPack components are already pre-installed, so this step can be skipped.</p> <pre><code>sudo apt update\nsudo apt install -y nvidia-jetpack\n</code></pre> </li> <li> <p>Restart the Docker service and add your user to the <code>docker</code> group, so that you don't need to use the command with <code>sudo</code>.</p> <pre><code>sudo systemctl restart docker\nsudo usermod -aG docker $USER\nnewgrp docker\n</code></pre> </li> <li> <p>Add default runtime in <code>/etc/docker/daemon.json</code></p> <pre><code>sudo vi /etc/docker/daemon.json\n</code></pre> <p>Insert the <code>\"default-runtime\": \"nvidia\"</code> line as following:</p> <pre><code>{\n\"runtimes\": {\n\"nvidia\": {\n\"path\": \"nvidia-container-runtime\",\n\"runtimeArgs\": []\n}\n},\n\"default-runtime\": \"nvidia\"\n}\n</code></pre> </li> <li> <p>Restart Docker</p> <pre><code>sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart docker\n</code></pre> </li> </ol>"},{"location":"tips_ssd-docker.html#migrate-docker-directory-to-ssd","title":"Migrate Docker directory to SSD","text":"<p>Now that the SSD is installed and available on your device, you can use the extra storage capacity to hold the storage-demanding Docker directory.</p> <ol> <li> <p>Stop the Docker service.</p> <pre><code>sudo systemctl stop docker\n</code></pre> </li> <li> <p>Move the existing Docker folder</p> <pre><code>sudo du -csh /var/lib/docker/ &amp;&amp; \\\nsudo mkdir /ssd/docker &amp;&amp; \\\nsudo rsync -axPS /var/lib/docker/ /ssd/docker/ &amp;&amp; \\\nsudo du -csh  /ssd/docker/ </code></pre> </li> <li> <p>Edit <code>/etc/docker/daemon.json</code></p> <pre><code>sudo vi /etc/docker/daemon.json\n</code></pre> <p>Insert <code>\"data-root\"</code> line like the following.</p> <pre><code>{\n\"runtimes\": {\n\"nvidia\": {\n\"path\": \"nvidia-container-runtime\",\n\"runtimeArgs\": []\n}\n},\n\"default-runtime\": \"nvidia\",\n\"data-root\": \"/ssd/docker\"\n}\n</code></pre> </li> <li> <p>Rename the old Docker data directory</p> <pre><code>sudo mv /var/lib/docker /var/lib/docker.old\n</code></pre> </li> <li> <p>Restart the docker daemon</p> <pre><code>sudo systemctl daemon-reload &amp;&amp; \\\nsudo systemctl restart docker &amp;&amp; \\\nsudo journalctl -u docker\n</code></pre> </li> </ol>"},{"location":"tips_ssd-docker.html#test-docker-on-ssd","title":"Test Docker on SSD","text":"<ol> <li> <p>[Terminal 1] First, open a terminal to monitor the disk usage while pulling a Docker image.</p> <pre><code>watch -n1 df </code></pre> </li> <li> <p>[Terminal 2] Next, open a new terminal and start Docker pull.</p> <pre><code>docker pull nvcr.io/nvidia/l4t-base:r35.2.1\n</code></pre> </li> <li> <p>[Terminal 1] Observe that the disk usage on <code>/ssd</code> goes up as the container image is downloaded and extracted.</p> <pre><code>~$ docker image ls\nREPOSITORY                  TAG       IMAGE ID       CREATED        SIZE\nnvcr.io/nvidia/l4t-base     r35.2.1   dc07eb476a1d   7 months ago   713MB\n</code></pre> </li> </ol>"},{"location":"tips_ssd-docker.html#final-verification","title":"Final Verification","text":"<p>Reboot your Jetson, and verify that you observe the following:</p> <pre><code>~$ sudo blkid | grep nvme\n/dev/nvme0n1: UUID=\"9fc06de1-7cf3-43e2-928a-53a9c03fc5d8\" TYPE=\"ext4\"\n\n~$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/mmcblk1p1  116G   18G   94G  16% /\nnone            3.5G     0  3.5G   0% /dev\ntmpfs           3.6G  108K  3.6G   1% /dev/shm\ntmpfs           734M   35M  699M   5% /run\ntmpfs           5.0M  4.0K  5.0M   1% /run/lock\ntmpfs           3.6G     0  3.6G   0% /sys/fs/cgroup\ntmpfs           734M   88K  734M   1% /run/user/1000\n/dev/nvme0n1    458G  824M  434G   1% /ssd\n\n~$ docker info | grep Root\n Docker Root Dir: /ssd/docker\n\n~$ sudo ls -l /ssd/docker/\ntotal 44\ndrwx--x--x  4 root root 4096 Mar 22 11:44 buildkit\ndrwx--x---  2 root root 4096 Mar 22 11:44 containers\ndrwx------  3 root root 4096 Mar 22 11:44 image\ndrwxr-x---  3 root root 4096 Mar 22 11:44 network\ndrwx--x--- 13 root root 4096 Mar 22 16:20 overlay2\ndrwx------  4 root root 4096 Mar 22 11:44 plugins\ndrwx------  2 root root 4096 Mar 22 16:19 runtimes\ndrwx------  2 root root 4096 Mar 22 11:44 swarm\ndrwx------  2 root root 4096 Mar 22 16:20 tmp\ndrwx------  2 root root 4096 Mar 22 11:44 trust\ndrwx-----x  2 root root 4096 Mar 22 16:19 volumes\n\n~$ sudo du -chs /ssd/docker/\n752M    /ssd/docker/\n752M    total\n\n~$ docker info | grep -e \"Runtime\" -e \"Root\"\nRuntimes: io.containerd.runtime.v1.linux nvidia runc io.containerd.runc.v2\n Default Runtime: nvidia\n Docker Root Dir: /ssd/docker\n</code></pre> <p>Your Jetson is now set up with the SSD!</p>"},{"location":"try.html","title":"Try","text":"<p>Jump to Sophgo.</p>"},{"location":"tutorial-intro.html","title":"Tutorial - Intro","text":""},{"location":"tutorial-intro.html#about-nvidia-jetson","title":"About NVIDIA Jetson","text":"<p>Note</p> <p>We are mainly targeting Jetson Orin generation devices for deploying the latest LLMs and generative AI models.</p> Jetson AGX Orin 64GB Developer Kit Jetson AGX Orin Developer Kit Jetson Orin Nano Developer Kit GPU 2048-core NVIDIA Ampere architecture GPU with 64 Tensor Cores 1024-core NVIDIA Ampere architecture GPU with 32 Tensor Cores RAM(CPU+GPU) 64GB 32GB 8GB Storage 64GB eMMC (+ NVMe SSD) microSD card (+ NVMe SSD)"},{"location":"tutorial_distillation.html","title":"CLIP model distillation","text":"<p>See \"Jetson Introduction to Knowledge Distillation\" repo's README.md.</p> <p>https://github.com/NVIDIA-AI-IOT/jetson-intro-to-distillation</p>"},{"location":"tutorial_llava.html","title":"Tutorial - LLaVA","text":"<p>Give your locally running LLM an access to vision, by running LLaVA on Jetson!</p> <p></p>"},{"location":"tutorial_llava.html#clone-and-set-up-jetson-containers","title":"Clone and set up <code>jetson-containers</code>","text":"<pre><code>git clone https://github.com/dusty-nv/jetson-containers\ncd jetson-containers\nsudo apt update; sudo apt install -y python3-pip\npip3 install -r requirements.txt\n</code></pre>"},{"location":"tutorial_llava.html#1-use-text-generation-webui-container-to-test-llava-model","title":"1. Use <code>text-generation-webui</code> container to test Llava model","text":"<p>What you need</p> <ol> <li> <p>One of the following Jetson:</p> <p>Jetson AGX Orin 64GB Jetson AGX Orin (32GB)</p> </li> <li> <p>Running one of the following JetPack.5x</p> <p>JetPack 5.1.2 (L4T r35.4.1) JetPack 5.1.1 (L4T r35.3.1) JetPack 5.1 (L4T r35.2.1)</p> </li> <li> <p>Sufficient storage space (preferably with NVMe SSD).</p> <ul> <li><code>6.2GB</code> for <code>text-generation-webui</code> container image</li> <li>Space for models<ul> <li>CLIP model : <code>1.7GB</code></li> <li>Llava-Llama2 merged model : <code>7.3GB</code></li> </ul> </li> </ul> </li> </ol>"},{"location":"tutorial_llava.html#use-text-generation-webui-container-for-web-ui","title":"Use <code>text-generation-webui</code> container for web UI","text":"<p>Using the <code>multimodal</code> extension, you can use the LLaVA model in oobaboonga's <code>text-generation-webui</code>.</p> <pre><code>./run.sh $(./autotag text-generation-webui) /bin/bash -c \\\n  \"python3 /opt/text-generation-webui/download-model.py \\\n  --output=/data/models/text-generation-webui \\\n  liuhaotian/llava-llama-2-13b-chat-lightning-gptq\"\n./run.sh $(./autotag text-generation-webui) /bin/bash -c \\\n  \"cd /opt/text-generation-webui &amp;&amp; python3 server.py --listen \\\n    --model-dir=/data/models/text-generation-webui \\\n    --model=liuhaotian_llava-llama-2-13b-chat-lightning-gptq \\\n    --multimodal-pipeline=llava-llama-2-13b \\\n    --extensions=multimodal \\\n    --chat \\\n    --verbose\"\n</code></pre> <p>Go to Chat tab, drag and drop an image of your choice into the Drop Image Here area, and your question in the text area above and hit Generate button.</p> <p></p>"},{"location":"tutorial_llava.html#result","title":"Result","text":""},{"location":"tutorial_llava.html#2-use-llava-container-to-run-llavaservecli","title":"2. Use <code>llava</code> container to run <code>llava.serve.cli</code>","text":"<p>What you need</p> <ol> <li> <p>One of the following Jetson:</p> <p>Jetson AGX Orin 64GB Jetson AGX Orin (32GB)</p> </li> <li> <p>Running one of the following JetPack.5x</p> <p>JetPack 5.1.2 (L4T r35.4.1) JetPack 5.1.1 (L4T r35.3.1) JetPack 5.1 (L4T r35.2.1)</p> </li> <li> <p>Sufficient storage space (preferably with NVMe SSD).</p> <ul> <li><code>6.1GB</code> for <code>llava</code> container image</li> <li>Space for models<ul> <li>7B model : <code>14GB</code>, or</li> <li>13B model : <code>26GB</code></li> </ul> </li> </ul> </li> </ol> <p>See <code>jetson-containers</code>' <code>llava</code> package README for more infomation**</p>"},{"location":"tutorial_llava.html#llava-llama-2-7b-chat","title":"llava-llama-2-7b-chat","text":"<pre><code>./run.sh --env HUGGING_FACE_HUB_TOKEN=&lt;YOUR-ACCESS-TOKEN&gt; $(./autotag llava) \\\n  python3 -m llava.serve.cli \\\n    --model-path liuhaotian/llava-llama-2-7b-chat-lightning-lora-preview \\\n    --model-base meta-llama/Llama-2-7b-chat-hf \\\n    --image-file /data/images/hoover.jpg\n</code></pre>"},{"location":"tutorial_llava.html#llava-llama-2-13b-chat","title":"llava-llama-2-13b-chat","text":"<p>This may only run on Jetson AGX Orin 64GB.</p> <pre><code>./run.sh $(./autotag llava) \\\n  python3 -m llava.serve.cli \\\n    --model-path liuhaotian/llava-llama-2-13b-chat-lightning-preview \\\n    --image-file /data/images/hoover.jpg\n</code></pre>"},{"location":"tutorial_minigpt4.html","title":"Tutorial - MiniGPT-4","text":"<p>Give your locally running LLM an access to vision, by running MiniGPT-4 on Jetson!</p> <p></p> <p>What you need</p> <ol> <li> <p>One of the following Jetson:</p> <p>Jetson AGX Orin 64GB Jetson AGX Orin (32GB) Jetson Orin Nano Orin (8GB)</p> </li> <li> <p>Running one of the following JetPack.5x</p> <p>JetPack 5.1.2 (L4T r35.4.1) JetPack 5.1.1 (L4T r35.3.1) JetPack 5.1 (L4T r35.2.1)</p> </li> <li> <p>Sufficient storage space (preferably with NVMe SSD).</p> <ul> <li><code>5.8GB</code> for container image</li> <li>Space for pre-quantized MiniGPT-4 model</li> </ul> </li> </ol>"},{"location":"tutorial_minigpt4.html#set-up-a-container-for-minigpt-4","title":"Set up a container for <code>MiniGPT-4</code>","text":"<p>See <code>jetson-containers</code>' <code>minigpt4</code> package README for more infomation**</p>"},{"location":"tutorial_minigpt4.html#clone-and-set-up-jetson-containers","title":"Clone and set up <code>jetson-containers</code>","text":"<pre><code>git clone https://github.com/dusty-nv/jetson-containers\ncd jetson-containers\nsudo apt update; sudo apt install -y python3-pip\npip3 install -r requirements.txt\n</code></pre>"},{"location":"tutorial_minigpt4.html#start-minigpt4-container-with-models","title":"Start <code>minigpt4</code> container with models","text":"<p>To start the MiniGPT4 container and webserver with the recommended models, run this command:</p> <pre><code>cd jetson-containers\n./run.sh $(./autotag minigpt4) /bin/bash -c 'cd /opt/minigpt4.cpp/minigpt4 &amp;&amp; python3 webui.py \\\n  $(huggingface-downloader --type=dataset maknee/minigpt4-13b-ggml/minigpt4-13B-f16.bin) \\\n  $(huggingface-downloader --type=dataset maknee/ggml-vicuna-v0-quantized/ggml-vicuna-13B-v0-q5_k.bin)'\n</code></pre> <p>Then, open your web browser and access <code>http://&lt;IP_ADDRESS&gt;:7860</code>.</p>"},{"location":"tutorial_minigpt4.html#results","title":"Results","text":""},{"location":"tutorial_nanodb.html","title":"Tutorial - NanoDB","text":"<p>Let's run NanoDB's interactive demo to witness the impact of Vector Database that handles multimodal data.</p> <p></p> <p>What you need</p> <ol> <li> <p>One of the following Jetson:</p> <p>Jetson AGX Orin 64GB Jetson AGX Orin (32GB)</p> </li> <li> <p>Running one of the following JetPack.5x</p> <p>JetPack 5.1.2 (L4T r35.4.1) JetPack 5.1.1 (L4T r35.3.1) JetPack 5.1 (L4T r35.2.1)</p> </li> <li> <p>Sufficient storage space (preferably with NVMe SSD).</p> <ul> <li><code>7.0GB</code> for container image</li> </ul> </li> </ol>"},{"location":"tutorial_nanodb.html#set-up-a-container-for-nanodb","title":"Set up a container for <code>nanodb</code>","text":""},{"location":"tutorial_nanodb.html#clone-jetson-containers","title":"Clone <code>jetson-containers</code>","text":"<p>See <code>jetson-containers</code>' <code>nanodb</code> package README for more infomation**</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\ncd jetson-containers\nsudo apt update; sudo apt install -y python3-pip\npip3 install -r requirements.txt\n</code></pre>"},{"location":"tutorial_nanodb.html#how-to-start","title":"How to start","text":""},{"location":"tutorial_nanodb.html#download-your-data","title":"Download your data","text":"<p>Just for an example, let's just use MS COCO dataset.</p> <pre><code>cd jetson-containers\nmkdir data/datasets/coco/\ncd data/datasets/coco\nwget http://images.cocodataset.org/zips/train2017.zip\nunzip train2017.zip\n</code></pre>"},{"location":"tutorial_nanodb.html#indexing-data","title":"Indexing Data","text":"<p>First, we need to build the index by scanning your dataset directory.</p> <pre><code>cd jetson-containers\n./run.sh -v ${PWD}/data/datasets/coco:/my_dataset $(./autotag nanodb) \\\n  python3 -m nanodb \\\n    --scan /my_dataset \\\n    --path /my_dataset/nanodb \\\n    --autosave --validate \n</code></pre> <p>This will take about 2 hours.</p> <p>Once the database has loaded and completed any start-up operations , it will drop down to a <code>&gt;</code> prompt from which the user can run search queries. You can quickly check the operation by typing your query on this prompt.</p> <pre><code>&gt; a girl riding a horse\n\n* index=80110   /data/datasets/coco/2017/train2017/000000393735.jpg      similarity=0.29991915822029114\n* index=158747  /data/datasets/coco/2017/unlabeled2017/000000189708.jpg  similarity=0.29254037141799927\n* index=123846  /data/datasets/coco/2017/unlabeled2017/000000026239.jpg  similarity=0.292171448469162\n* index=127338  /data/datasets/coco/2017/unlabeled2017/000000042508.jpg  similarity=0.29118549823760986\n* index=77416   /data/datasets/coco/2017/train2017/000000380634.jpg      similarity=0.28964102268218994\n* index=51992   /data/datasets/coco/2017/train2017/000000256290.jpg      similarity=0.28929752111434937\n* index=228640  /data/datasets/coco/2017/unlabeled2017/000000520381.jpg  similarity=0.28642547130584717\n* index=104819  /data/datasets/coco/2017/train2017/000000515895.jpg      similarity=0.285491943359375\n</code></pre> <p>You can press Ctrl+C to exit from the app and the container.</p>"},{"location":"tutorial_nanodb.html#interactive-web-ui","title":"Interactive web UI","text":"<p>Spin up the Gradio server.</p> <pre><code>cd jetson-containers\n./run.sh -v ${PWD}/data/datasets/coco:/my_dataset $(./autotag nanodb) \\\n  python3 -m nanodb \\\n    --path /my_dataset/nanodb \\\n    --server --port=7860\n</code></pre> <p>You can use your PC (or any machine) that can access your Jetson via a network, and navigate your browser to <code>http://&lt;IP_ADDRESS&gt;:7860</code></p> <p>You can enter text search queries as well as drag/upload images.</p>"},{"location":"tutorial_nanoowl.html","title":"Tutorial - NanoOWL","text":"<p>Check out the GitHub repo, https://github.com/NVIDIA-AI-IOT/nanoowl.</p> <p></p>"},{"location":"tutorial_nanosam.html","title":"Tutorial - NanoSAM","text":"<p>Let's run NVIDIA's NanoSAM to check out the performance gain by distillation.</p> <p></p> <p>What you need</p> <ol> <li> <p>One of the following Jetson:</p> <p>Jetson AGX Orin 64GB Jetson AGX Orin (32GB) Jetson Orin Nano Orin (8GB)</p> </li> <li> <p>Running one of the following JetPack.5x</p> <p>JetPack 5.1.2 (L4T r35.4.1) JetPack 5.1.1 (L4T r35.3.1) JetPack 5.1 (L4T r35.2.1)</p> </li> <li> <p>Sufficient storage space (preferably with NVMe SSD).</p> <ul> <li><code>6.3GB</code> for container image</li> <li>Spaces for models</li> </ul> </li> </ol>"},{"location":"tutorial_nanosam.html#set-up-a-container-for-nanosam","title":"Set up a container for <code>nanosam</code>","text":""},{"location":"tutorial_nanosam.html#clone-jetson-containers","title":"Clone <code>jetson-containers</code>","text":"<p>See <code>jetson-containers</code>' <code>nanosam</code> package README for more infomation**</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\ncd jetson-containers\nsudo apt update; sudo apt install -y python3-pip\npip3 install -r requirements.txt\n</code></pre>"},{"location":"tutorial_nanosam.html#how-to-start","title":"How to start","text":"<p>Use <code>run.sh</code> and <code>autotag</code> script to automatically pull or build a compatible container image.</p> <pre><code>cd jetson-containers\n./run.sh $(./autotag nanosam)\n</code></pre>"},{"location":"tutorial_nanosam.html#run-examples","title":"Run examples","text":"<p>Inside the container, you can move to <code>/opt/nanosam</code> directory, to go through all the examples demonstrated on the repo.</p> <pre><code>cd /opt/nanosam\n</code></pre> <p>To run the \"Example 1 - Segment with bounding box\":</p> <pre><code>python3 examples/basic_usage.py \\\n    --image_encoder=\"data/resnet18_image_encoder.engine\" \\\n    --mask_decoder=\"data/mobile_sam_mask_decoder.engine\"\n</code></pre> <p>The result is saved under <code>/opt/nanosam/data/basic_usage_out.jpg</code>.</p> <p>To check on your host machine, you can copy that into <code>/data</code> directory of the container where that is mounted from the host.</p> <pre><code>cp data/basic_usage_out.jpg /data/\n</code></pre> <p>Then you can go to your host system, and find the file under the <code>jetson_containers</code>' <code>data</code> directory, like <code>jetson_containers/data/basic_usage_out.jpg</code>.</p>"},{"location":"tutorial_nanosam.html#results","title":"Results","text":""},{"location":"tutorial_sam.html","title":"Tutorial - SAM (Segment Anything)","text":"<p>Let's run Meta's <code>SAM</code> on NVIDIA Jetson.</p> <p></p> <p>What you need</p> <ol> <li> <p>One of the following Jetson:</p> <p>Jetson AGX Orin 64GB Jetson AGX Orin (32GB) Jetson Orin Nano Orin (8GB)\u26a0\ufe0f1</p> </li> <li> <p>Running one of the following JetPack.5x</p> <p>JetPack 5.1.2 (L4T r35.4.1) JetPack 5.1.1 (L4T r35.3.1) JetPack 5.1 (L4T r35.2.1)</p> </li> <li> <p>Sufficient storage space (preferably with NVMe SSD).</p> <ul> <li><code>6.8GB</code> for container image</li> <li>Spaces for models</li> </ul> </li> </ol>"},{"location":"tutorial_sam.html#set-up-a-container-for-sam","title":"Set up a container for <code>sam</code>","text":""},{"location":"tutorial_sam.html#clone-jetson-containers","title":"Clone <code>jetson-containers</code>","text":"<p>See <code>jetson-containers</code>' <code>sam</code> package README for more infomation**</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\ncd jetson-containers\nsudo apt update; sudo apt install -y python3-pip\npip3 install -r requirements.txt\n</code></pre>"},{"location":"tutorial_sam.html#how-to-start","title":"How to start","text":"<p>Use <code>run.sh</code> and <code>autotag</code> script to automatically pull or build a compatible container image.</p> <pre><code>cd jetson-containers\n./run.sh $(./autotag sam)\n</code></pre> <p>The container has a default run command (<code>CMD</code>) that will automatically start the Jupyter Lab server.</p> <p>Open your browser and access <code>http://&lt;IP_ADDRESS&gt;:8888</code>.</p> <p>The default password for Jupyter Lab is <code>nvidia</code>.</p>"},{"location":"tutorial_sam.html#run-jupyter-notebook","title":"Run Jupyter notebook","text":"<p>In Jupyter Lab, navigate to <code>notebooks</code> and open <code>automatic_mask_generator_example.py</code> notebook.</p> <p>Create a new cell at the top, insert the model download command below and run the cell.</p> <pre><code>!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n</code></pre> <p>Then go through executing all the cells below Set-up.</p>"},{"location":"tutorial_sam.html#results","title":"Results","text":"<ol> <li> <p>The biggest <code>vit_h</code> (2.4GB) model may not ran due to OOM, but <code>vit_l</code> (1.1GB) runs on Jetson Orin Nano.\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorial_stable-diffusion-xl.html","title":"Tutorial - Stable Diffusion XL","text":"<p>Stable Diffusion XL is a newer ensemble pipeline consisting of a base model and refiner that results in significantly enhanced and detailed image generation capabilities.  All told, SDXL 1.0 has 6.6 billion model parameters, in comparison to 0.98 billion for the original SD 1.5 model.</p> <p></p> <p>What you need</p> <ol> <li> <p>One of the following Jetson's (SDXL requires &gt;= ~13GB memory)</p> <p>Jetson AGX Orin 64GB Jetson AGX Orin (32GB)</p> </li> <li> <p>Running one of the following versions of JetPack 5</p> <p>JetPack 5.1.2 (L4T r35.4.1) JetPack 5.1.1 (L4T r35.3.1) JetPack 5.1 (L4T r35.2.1)</p> </li> <li> <p>Sufficient storage space (preferably with NVMe SSD).</p> <ul> <li><code>6.8GB</code> for container image</li> <li><code>12.4GB</code> for SDXL models</li> </ul> </li> <li> <p>Have followed the previous <code>stable-diffusion-webui</code> tutorial and have the webserver container running.</p> </li> </ol>"},{"location":"tutorial_stable-diffusion-xl.html#downloading-sdxl-models","title":"Downloading SDXL Models","text":"<p>Stable Diffusion XL is supported through AUTOMATIC1111's <code>stable-diffusion-webui</code> with some additional settings.  First you need to download the SDXL models to your <code>jetson-containers</code> data directory (which is automatically mounted into the container)</p> <pre><code># run these outside of container, and replace CONTAINERS_DIR with the path to the jetson-containers repo on your device\nCONTAINERS_DIR=/path/to/your/jetson-containers\nMODEL_DIR=$CONTAINERS_DIR/data/models/stable-diffusion/models/Stable-diffusion/\n\nsudo chown -R $USER $MODEL_DIR\n\nwget -P $MODEL_DIR https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors\nwget -P $MODEL_DIR https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/resolve/main/sd_xl_refiner_1.0.safetensors\n</code></pre> <p>It's assumed that you already have the <code>stable-diffusion-webui</code> container and webserver running from the previous tutorial.</p>"},{"location":"tutorial_stable-diffusion-xl.html#sdxl-settings","title":"SDXL Settings","text":"<p>After the models have finished downloading, click the \ud83d\udd04 button to refresh the model list in the web UI.  Select <code>sd_xl_base_1.0.safetensors</code> from the Stable Diffusion checkpoint drop-down:</p> <p></p> <p>Then under the Generation tab, expand the Refiner section, and select <code>sd_xl_refiner_1.0.safetensors</code> from the drop-down:</p> <p></p> <p>Guidance on other relevant settings:</p> <ul> <li>Change the width/height to 1024x1024.  SDXL works best at higher resolutions, and using 512x512 often results in more simplistic/cartoonish content.  Changing image resolutions impacts the actual scene contents of the image, not just the details.</li> <li>The refiner's <code>Switch at</code> setting dictates the step at which the refiner takes over from the base model.  At this point, additional subject content will no longer be added to the scene, but rather its details further refined in the image.</li> <li>Typical <code>Sampling steps</code> are between 20-40 and <code>Switch at</code> is between 0.7-0.9.  This takes experimentation to find the best combination for the characteristics of your desired output.</li> <li>Extensive negative prompts are not as necessary in the same way as SD 1.5 was (e.g. <code>low quality, jpeg artifacts, blurry, ect</code>)</li> <li><code>CFG Scale</code> controls how closely the model conforms to your prompt versus how creative it is.</li> </ul> <p>When you get a good image, remember to save your random seed and settings so you can re-generate it later!</p>"},{"location":"tutorial_stable-diffusion-xl.html#results","title":"Results","text":"<p> photograph of a friendly robot alongside a person climbing a mountain (seed 1576166644, steps 25, switch @ 0.8, cfg scale 15)</p> <p> a girl and a boy building a friendly robot in their basement workshop (seed 642273464, steps 25, switch @ 0.9, cfg scale 7)</p> <p> small friendly robots playing games with people, in a futuristic Tokyo central park gardens with cherry blossoms and water, coy fish swimming in the water, sunshine (seed 642273464, steps 40, switch @ 0.8, cfg scale 7)</p> <p> small friendly robots playing games with people in a futuristic New York City Central Park in autumn, water (seed 642273464, steps 25, switch @ 0.8, cfg scale 7)</p> <p>Want to explore using Python APIs to run diffusion models directly? See <code>jetson-containers/stable-diffusion</code>.</p>"},{"location":"tutorial_stable-diffusion.html","title":"Tutorial - Stable Diffusion","text":"<p>Let's run AUTOMATIC1111's <code>stable-diffusion-webui</code> on NVIDIA Jetson to generate images from our prompts!</p> <p></p> <p>What you need</p> <ol> <li> <p>One of the following Jetson's:</p> <p>Jetson AGX Orin 64GB Jetson AGX Orin (32GB) Jetson Orin Nano Orin (8GB)</p> </li> <li> <p>Running one of the following versions of JetPack 5</p> <p>JetPack 5.1.2 (L4T r35.4.1) JetPack 5.1.1 (L4T r35.3.1) JetPack 5.1 (L4T r35.2.1)</p> </li> <li> <p>Sufficient storage space (preferably with NVMe SSD).</p> <ul> <li><code>6.8GB</code> for container image</li> <li><code>4.1GB</code> for SD 1.5 model</li> </ul> </li> </ol>"},{"location":"tutorial_stable-diffusion.html#set-up-a-container-for-stable-diffusion-webui","title":"Set up a container for stable-diffusion-webui","text":"<p>The jetson-containers project provides pre-built Docker images for <code>stable-diffusion-webui</code>.  You can clone the repo to use its utilities that will automatically pull/start the correct container for you, or you can do it manually.</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\ncd jetson-containers\nsudo apt update; sudo apt install -y python3-pip\npip3 install -r requirements.txt\n</code></pre> <p>Info</p> <p>JetsonHacks provides an informative walkthrough video on <code>jetson-containers</code>, showcasing the usage of both the <code>stable-diffusion-webui</code> and <code>text-generation-webui</code> containers. You can find the complete article with detailed instructions here.</p> <p></p>"},{"location":"tutorial_stable-diffusion.html#how-to-start","title":"How to start","text":"<p>If you are running this for the first time, go through the pre-setup and see the <code>jetson-containers/stable-diffusion-webui</code> readme.</p> <p>Use <code>run.sh</code> and <code>autotag</code> script to automatically pull or build a compatible container image:</p> <pre><code>cd jetson-containers\n./run.sh $(./autotag stable-diffusion-webui)\n</code></pre> <p>The container has a default run command (<code>CMD</code>) that will automatically start the webserver like this:</p> <pre><code>cd /opt/stable-diffusion-webui &amp;&amp; python3 launch.py \\\n  --data=/data/models/stable-diffusion \\\n  --enable-insecure-extension-access \\\n  --xformers \\\n  --listen \\\n  --port=7860\n</code></pre> <p>You should see it downloading the model checkpoint on the first run.</p> <p>Open your browser and access <code>http://&lt;IP_ADDRESS&gt;:7860</code></p>"},{"location":"tutorial_stable-diffusion.html#results-output-examples","title":"Results / Output Examples","text":""},{"location":"tutorial_stable-diffusion.html#stable-diffusion-xl","title":"Stable Diffusion XL","text":"<p>To generate even higher-quality and detailed images, check out the next part of the tutorial that uses the latest Stable Diffusion XL models!</p> <p>Want to explore using Python APIs to run diffusion models directly? See <code>jetson-containers/stable-diffusion</code>.</p>"},{"location":"tutorial_tam.html","title":"Tutorial - SAM (Segment Anything)","text":"<p>Let's run <code>TAM</code> to perform Segment Anything on videos on NVIDIA Jetson.</p> <p></p> <p>What you need</p> <ol> <li> <p>One of the following Jetson:</p> <p>Jetson AGX Orin 64GB Jetson AGX Orin (32GB)</p> </li> <li> <p>Running one of the following JetPack.5x</p> <p>JetPack 5.1.2 (L4T r35.4.1) JetPack 5.1.1 (L4T r35.3.1) JetPack 5.1 (L4T r35.2.1)</p> </li> <li> <p>Sufficient storage space (preferably with NVMe SSD).</p> <ul> <li><code>6.8GB</code> for container image</li> <li>Spaces for models</li> </ul> </li> </ol>"},{"location":"tutorial_tam.html#set-up-a-container-for-tam","title":"Set up a container for <code>tam</code>","text":""},{"location":"tutorial_tam.html#clone-jetson-containers","title":"Clone <code>jetson-containers</code>","text":"<p>See <code>jetson-containers</code>' <code>tam</code> package README for more infomation**</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\ncd jetson-containers\nsudo apt update; sudo apt install -y python3-pip\npip3 install -r requirements.txt\n</code></pre>"},{"location":"tutorial_tam.html#how-to-start","title":"How to start","text":"<p>Use <code>run.sh</code> and <code>autotag</code> script to automatically pull or build a compatible container image.</p> <pre><code>cd jetson-containers\n./run.sh $(./autotag tam)\n</code></pre> <p>The container has a default run command (<code>CMD</code>) that will automatically start TAM's web server.</p> <p>Open your browser and access <code>http://&lt;IP_ADDRESS&gt;:12212</code>.</p>"},{"location":"tutorial_tam.html#tam-web-ui","title":"TAM web UI","text":"<p>Check out the official tutorial to learn how to operate the web UI.</p>"},{"location":"tutorial_tam.html#results","title":"Results","text":""},{"location":"tutorial_text-generation.html","title":"Tutorial - text-generation-webui","text":"<p>Interact with a local AI assistant by running a LLM with oobabooga's <code>text-generaton-webui</code> on NVIDIA Jetson!</p> <p></p> <p>What you need</p> <ol> <li> <p>One of the following Jetson:</p> <p>Jetson AGX Orin 64GB Jetson AGX Orin (32GB) Jetson Orin Nano Orin (8GB)\u26a0\ufe0f1</p> </li> <li> <p>Running one of the following JetPack.5x</p> <p>JetPack 5.1.2 (L4T r35.4.1) JetPack 5.1.1 (L4T r35.3.1) JetPack 5.1 (L4T r35.2.1)</p> </li> <li> <p>Sufficient storage space (preferably with NVMe SSD).</p> <ul> <li><code>6.2GB</code> for container image</li> <li>Spaces for models</li> </ul> </li> </ol>"},{"location":"tutorial_text-generation.html#set-up-a-container-for-text-generation-webui","title":"Set up a container for text-generation-webui","text":"<p>The jetson-containers project provides pre-built Docker images for <code>text-generation-webui</code> along with all of the loader API's built with CUDA enabled (llama.cpp, ExLlama, AutoGPTQ, Transformers, ect).  You can clone the repo to use its utilities that will automatically pull/start the correct container for you, or you can do it manually.</p> <pre><code>git clone --depth=1 https://github.com/dusty-nv/jetson-containers\ncd jetson-containers\nsudo apt update; sudo apt install -y python3-pip\npip3 install -r requirements.txt\n</code></pre> <p>Info</p> <p>JetsonHacks provides an informative walkthrough video on <code>jetson-containers</code>, showcasing the usage of both the <code>stable-diffusion-webui</code> and <code>text-generation-webui</code> containers.  You can find the complete article with detailed instructions here.</p> <p></p>"},{"location":"tutorial_text-generation.html#how-to-start","title":"How to start","text":"<p>If you are running this for the first time, go through the pre-setup and see the <code>jetson-containers/text-generation-webui</code> container readme.</p> <p>Use <code>run.sh</code> and <code>autotag</code> script to automatically pull or build a compatible container image:</p> <pre><code>cd jetson-containers\n./run.sh $(./autotag text-generation-webui)\n</code></pre> <p>The container has a default run command (<code>CMD</code>) that will automatically start the webserver like this:</p> <pre><code>cd /opt/text-generation-webui &amp;&amp; python3 server.py \\\n  --model-dir=/data/models/text-generation-webui \\\n  --chat \\\n  --listen\n</code></pre> <p>Open your browser and access <code>http://&lt;IP_ADDRESS&gt;:7860</code>.</p>"},{"location":"tutorial_text-generation.html#download-a-model-on-web-ui","title":"Download a model on web UI","text":"<p>See the oobabooga documentation for instructions for downloading models - either from within the web UI, or using <code>download-model.py</code></p> <pre><code>./run.sh --workdir=/opt/text-generation-webui $(./autotag text-generation-webui) /bin/bash -c \\\n'python3 download-model.py --output=/data/models/text-generation-webui TheBloke/Llama-2-7b-Chat-GPTQ'\n</code></pre> <p>From within the web UI, select Model tab and navigate to \"Download model or LoRA\" section.  </p> <p>You can find text generation models on Hugging Face Hub, then enter the Hugging Face username/model path (which you can have copied to your clipboard from the Hub).  Then click the Download button.</p>"},{"location":"tutorial_text-generation.html#gguf-models","title":"GGUF models","text":"<p>The fastest oobabooga model loader to use is currently llama.cpp with 4-bit quantized GGUF models.</p> <p>You can download a single model file for a particular quantization, like <code>*.Q4_K_M.bin</code>. Input the file name and hit Download button.</p> Model Quantization Memory (MB) <code>TheBloke/Llama-2-7b-Chat-GGUF</code> <code>llama-2-7b-chat.Q4_K_M.gguf</code> 5,268 <code>TheBloke/Llama-2-13B-chat-GGUF</code> <code>llama-2-13b-chat.Q4_K_M.gguf</code> 8,609 <code>TheBloke/LLaMA-30b-GGUF</code> <code>llama-30b.Q4_K_S.gguf</code> 19,045 <code>TheBloke/Llama-2-70B-chat-GGUF</code> <code>llama-2-70b-chat.Q4_K_M.gguf</code> 37,655 <p></p> <p>Info</p>"},{"location":"tutorial_text-generation.html#model-selection-for-jetson-orin-nano","title":"Model selection for Jetson Orin Nano","text":"<p>Jetson Orin Nano Developer Kit has only 8GB RAM for both CPU (system) and GPU, so you need to pick a model that fits in the RAM size - see the Model Size section below.  The 7B models with 4-bit quantization are the ones to use on Jetson Orin Nano.  Make sure you go through the RAM optimization steps before attempting to load such model on Jetson Orin Nano.</p>"},{"location":"tutorial_text-generation.html#load-a-model","title":"Load a model","text":"<p>After you have downloaded a model, click the \ud83d\udd04 button to refresh your model list, and select the model you want to use.</p> <p>For a GGUF model, remember to</p> <ul> <li>Set <code>n-gpu-layers</code> to <code>128</code></li> <li>Set <code>n_gqa</code> to <code>8</code> if you using Llama-2-70B (on Jetson AGX Orin 64GB)</li> </ul> <p>Then click the Load button.</p>"},{"location":"tutorial_text-generation.html#chat-template","title":"Chat Template","text":"<p>If you're using a Llama model fine-tuned for chat, like the models listed above (except for <code>LLaMA-30b</code>), you need to use the oobabooga Instruct mode and set the template.  On the Parameters tab, go to the Instruction Template sub-tab, then select <code>Llama-v2</code> from the Instruction Template drop-down (or Vicuna, Guanaco, ect if you are using one of those models)  </p> <p>For the base text completion models (like <code>LLaMA-30b</code>), use the Default or Notebook tab.</p> <p>Selecting the right chat template will make sure the model is being prompted correctly - you can also change the system prompt in the Context box to alter the agent's personality and behavior.  There are a lot of other settings under the Generation tab, like the maximum length it should output per reply, and token sampling parameters like <code>temperature</code> and <code>top_p</code> for controlling randomness.</p> <p>Then change back to the Chat tab, and under the mode section, make sure Instruct is selected (confusingly, not chat mode).  Then you can start chatting with the LLM!</p>"},{"location":"tutorial_text-generation.html#results","title":"Results","text":""},{"location":"tutorial_text-generation.html#things-to-do-with-your-llm","title":"Things to do with your LLM","text":"<p>Here are some common test prompts for coding, math, history ect.  You can also ask it about geography, travel, nature, recipies, fixing things, general life advice, and practically everything else.  Also Llama-2 is quite playful and likes to play games to test its logic abilities!</p> <pre><code>&gt;&gt; What games do you like to play?\n\nI'm a large language model, so I can play text-based games and answer questions on a wide variety of topics. Here are some game ideas:\n\n- 20 Questions:  You or I can think of an object, and the other player tries to guess what it is by asking up to 20 yes-or-no questions.  The goal is to guess the object in as few questions as possible.\n- Hangman: I can think of a word or phrase and you try to guess it by suggesting letters. Each incorrect guess will bring the hangman one step closer to being complete!\n- Word association: I'll give you a word and you have to come up with a word that is related to it. We can keep playing back and forth as long as you'd like!\n- Trivia: I can provide questions on a wide range of topics, from history to science to pop culture. See how many questions you can answer correctly!\n- Storytelling: I can start telling a story and then stop at a cliffhanger. You can then try to guess what happens next or even take over the storytelling and continue it in your own\n- Jokes: I love to tell jokes and make people laugh with my \"Dad humor\"!  Knock knock!  *giggles*\n</code></pre>"},{"location":"tutorial_text-generation.html#model-size-tested","title":"Model size tested","text":"<p>With llama.cpp, GGUF model, 4-bit quantization:</p> Model size Jetson AGX Orin 64GB Jetson AGX Orin 32GB Jetson Orin Nano 8GB 70B model \u2705 30B model \u2705 \u2705 13B model \u2705 \u2705 7B model \u2705 \u2705 \u2705 <p>Want to explore using Python APIs to run LLMs directly?  See <code>jetson-containers</code> for its LLM related packages and containers.</p> <ol> <li> <p>Limited to 7B model (4-bit quantized).\u00a0\u21a9</p> </li> </ol>"},{"location":"airbox/index.html","title":"Welcome to AirBox","text":"<p>\u8fd9\u662fAirBox\u7684\u64cd\u4f5c\u624b\u518c\u3002\u672c\u624b\u518c\u4ecb\u7ecd\u4e86\u5982\u4f55\u5237\u673a\u3001\u8fde\u63a5AirBox\uff0c\u4ee5\u53ca\u5982\u4f55\u5b89\u88c5\u4f7f\u7528ChatDoc\u548cChatBot\u5e94\u7528\u3002</p>"},{"location":"airbox/AirBox%E8%BF%9E%E6%8E%A5%E6%96%B9%E5%BC%8F.html","title":"AirBox\u8fde\u63a5\u65b9\u5f0f","text":""},{"location":"airbox/AirBox%E8%BF%9E%E6%8E%A5%E6%96%B9%E5%BC%8F.html#_1","title":"\u4e32\u53e3\u8fde\u63a5","text":"<p>\u4e32\u53e3\u8fde\u63a5\u662f\u4e00\u79cd\u7528\u4e8e\u5728\u8ba1\u7b97\u673a\u548c\u5916\u90e8\u8bbe\u5907\u4e4b\u95f4\u8fdb\u884c\u6570\u636e\u4f20\u8f93\u7684\u901a\u4fe1\u63a5\u53e3\u3002\u4e0d\u540c\u4e0e\u7f51\u7edc\u8fde\u63a5\uff0c\u4e32\u53e3\u4e0d\u9700\u8981\u590d\u6742\u7684\u914d\u7f6e\u5373\u53ef\u5b8c\u6210\u901a\u4fe1\uff0c\u56e0\u6b64\u5728\u6545\u969c\u8c03\u8bd5\u7b49\u65b9\u9762\u662f\u4e00\u79cd\u91cd\u8981\u7684\u8fde\u63a5\u65b9\u5f0f\u3002AirBox\u7684\u4e32\u53e3\u4f4d\u4e8e\u673a\u8eab\u7684Type-C\u63a5\u53e3\u3002\u4e0b\u9762\u4ecb\u7ecdMac\u5e73\u53f0\u4f7f\u7528minicom\u8fdb\u884c\u4e32\u53e3\u901a\u4fe1\u3002</p> <ul> <li>minicom\u5b89\u88c5\u3002\u4f7f\u7528<code>brew install minicom</code>\u5373\u53ef\u5b89\u88c5minicom\u3002</li> <li>\u901a\u8fc7\u7ebf\u7f06\u5c06AirBox\u7684Type-C\u63a5\u53e3\u548c\u8ba1\u7b97\u673a\u8fde\u63a5\u3002\u5728\u8ba1\u7b97\u673a\u7ec8\u7aef\u4e2d\u6267\u884c<code>ls /dev/tty.*</code>\u547d\u4ee4\u67e5\u770b\u4e32\u53e3\u540d\uff0c\u4e00\u822c\u542b\u6709usb\u7684\u5c5e\u4e8e\u4e0eAirBox\u76f8\u8fde\u7684\u4e32\u53e3\u540d\u3002 \u5982\u4e0a\u56fe\u4e2d\u7684/dev/tty.usbserial-0001\u5373\u4e3a\u6240\u9700\u4e32\u53e3\u540d\u3002</li> <li><code>minicom -s</code>\u8fdb\u5165\u8bbe\u7f6e\u83dc\u5355\uff0c\u9009\u62e9Serial port setup\u8fdb\u5165\u4e32\u53e3\u8bbe\u7f6e\u3002</li> <li>\u6309\u56de\u8f66\u8fd4\u56de\u5230\u83dc\u5355\u9875\uff0c\u7136\u540e\u9009\u62e9Exit\u5373\u53ef\u8fdb\u5165\u4e32\u53e3\u901a\u4fe1\u9875\u9762\u3002</li> </ul>"},{"location":"airbox/AirBox%E8%BF%9E%E6%8E%A5%E6%96%B9%E5%BC%8F.html#_2","title":"\u7f51\u7edc\u8fde\u63a5","text":"<p>AirBox\u62e5\u6709\u4e00\u4e2aLAN\u53e3\u548c\u4e00\u4e2aWAN\u53e3\u3002\u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528WAN\u53e3\u76f4\u63a5\u4e0e\u6211\u4eec\u7684\u8ba1\u7b97\u673a\u76f8\u8fde\uff1b\u6216\u8005\u6211\u4eec\u8fd8\u53ef\u4ee5\u901a\u8fc7LAN\u53e3\u63a5\u5165\u8def\u7531\u5668\u6216\u8fde\u63a5\u5171\u4eab\u7f51\u7edc\u7684\u8ba1\u7b97\u673a\uff1b\u4e0b\u9762\u5206\u522b\u6f14\u793a\u8fd9\u4e24\u79cd\u8fde\u63a5\u65b9\u5f0f\u3002</p>"},{"location":"airbox/AirBox%E8%BF%9E%E6%8E%A5%E6%96%B9%E5%BC%8F.html#wan","title":"WAN\u53e3\u8fde\u63a5","text":"<p>\u5c06AirBox\u4e0e\u8ba1\u7b97\u673a\u76f8\u8fde\uff0c\u7136\u540e\u8bbe\u7f6e\u8ba1\u7b97\u673a\u7aef\u7684IP\u5730\u5740\u3002\u5982\u4e0a\u56fe\u5c06IP\u83b7\u53d6\u65b9\u5f0f\u8bbe\u7f6e\u4e3a\u624b\u52a8\uff0cIP\u5730\u5740\u8bbe\u7f6e\u4e3a192.168.150.2\uff0c\u8fde\u63a5\u6210\u529f\u540e\uff0cAirBox\u7684IP\u5373\u662f192.168.150.1\u3002</p> <pre><code>\u6ce8\u610f\u8fd9\u79cd\u65b9\u5f0f\u8fde\u63a5AirBox\u5e76\u6ca1\u6709\u63a5\u5165\u4e92\u8054\u7f51\u3002\n</code></pre>"},{"location":"airbox/AirBox%E8%BF%9E%E6%8E%A5%E6%96%B9%E5%BC%8F.html#lan","title":"LAN\u53e3\u8fde\u63a5","text":"<p>\u5c06AirBox\u7684LAN\u53e3\u76f4\u63a5\u4e0e\u8def\u7531\u5668\u76f8\u8fde\uff0cAirBox\u5c06\u81ea\u52a8\u83b7\u53d6\u8def\u7531\u5668\u5206\u914d\u7684IP\u5730\u5740\uff0cIP\u5730\u5740\u901a\u8fc7\u8def\u7531\u5668\u6216\u8005\u4e32\u53e3\u8fdb\u5165AirBox\u7ec8\u7aef\u6267\u884c<code>ifconfig</code>\u67e5\u770b\u3002</p> <p>\u7531\u4e8eWAN\u53e3\u7684\u65b9\u5f0f\u8fde\u63a5AirBox\u4e0d\u80fd\u63a5\u5165\u4e92\u8054\u7f51\uff0c\u800c\u4f7f\u7528\u8def\u7531\u5668\u8fde\u63a5\u53ef\u80fd\u53d7\u9650\u4e8e\u8bbe\u5907\uff0c\u8fd9\u79cd\u60c5\u51b5\u6211\u4eec\u63a8\u8350\u4e00\u79cd\u4ec5\u901a\u8fc7\u8ba1\u7b97\u673a\u5c31\u53ef\u4ee5LAN\u53e3\u8fde\u63a5\u7684\u65b9\u5f0f\u3002\u4ee5MacOS\u4e3a\u4f8b\uff0c\u8fd9\u79cd\u65b9\u5f0f\u7684\u64cd\u4f5c\u6b65\u9aa4\u5982\u4e0b\u3002</p> <p></p> <ul> <li>\u6253\u5f00\u7cfb\u7edf\u8bbe\u7f6e-\u901a\u7528-\u5171\u4eab\uff0c\u7136\u540e\u6253\u5f00\u4e92\u8054\u7f51\u5171\u4eab\u7684\u914d\u7f6e\u5c06\u7f51\u7edc\u5171\u4eab\u5230\u4e0eAirBox\u76f8\u8fde\u7684\u7f51\u7edc\u8bbe\u5907\u4e0a\u3002\u5982\u4e0a\u56feUSB 10/100/1000 LAN\u5219\u662f\u4e0eAirBox\u76f8\u8fde\u7684\u8bbe\u5907\u3002\u70b9\u51fb\u5b8c\u6210\uff0c\u5f00\u59cb\u5171\u4eab\u7f51\u7edc\u3002</li> </ul> <p></p> <ul> <li>\u901a\u8fc7\u4e32\u53e3\u67e5\u770bAirBox\u7684IP\u5730\u5740\uff0c\u6216\u8005\u901a\u8fc7\u904d\u5386PING\u7684\u65b9\u6cd5\u3002\u5982\u5728\u8ba1\u7b97\u673a\u7ec8\u7aef\u91cc\u6267\u884c<code>for i in {1..255}; do ping -c 1 -W 1 192.168.2.$i &gt;/dev/null &amp;&amp; echo \"x.x.x.$i is online\"; done</code>\u67e5\u8be2AirBox\u7684IP\u5730\u5740\u3002\u4e0a\u56fe\u53ef\u77e5\uff0c192.168.2.4\u5373\u4e3aAirBox\u7684IP\u5730\u5740\u3002\u6b64\u65f6AirBox\u4e5f\u63a5\u5165\u4e86\u4e92\u8054\u7f51\u3002</li> </ul>"},{"location":"airbox/%E5%87%86%E5%A4%87.html","title":"\u51c6\u5907","text":""},{"location":"airbox/%E5%87%86%E5%A4%87.html#_1","title":"\u5237\u673a","text":"<ul> <li>\u5173\u673a\u72b6\u6001\u4e0b\u5c06TF\u5361\u63d2\u5165AirBox\u5361\u69fd\u3002</li> </ul> <ul> <li>\u901a\u7535\u5f00\u673a\uff0c\u5373\u53ef\u81ea\u52a8\u5237\u673a\u3002\u53ef\u901a\u8fc7\u4e32\u53e3\u8fde\u63a5\u67e5\u770b\u7cfb\u7edf\u5237\u673a\u72b6\u6001,\u5f53\u51fa\u73b0\u5982\u4e0a\u56fe\u8f93\u51fa\u7684\u65e5\u5fd7\u65f6\uff0c\u5219\u8868\u793a\u5237\u673a\u5b8c\u6210\uff0c\u62d4\u6389TF\u5361\u540e\u91cd\u542f\u5373\u53ef\u3002\u7ea610\u5206\u949f\u53ef\u5237\u673a\u5b8c\u6210\u3002</li> </ul>"},{"location":"airbox/%E5%87%86%E5%A4%87.html#_2","title":"\u4fee\u6539\u5185\u5b58\u5206\u5e03","text":"<p>\u7cfb\u7edf\u9ed8\u8ba4\u7684\u5185\u5b58\u5206\u914d\u662f\u7cfb\u7edf\u5185\u5b581GB\uff0cTPU\u4e13\u7528\u5185\u5b58\u662f15GB\u3002AirBox\u63d0\u4f9b\u4e09\u79cd\u9884\u8bbe\u7684\u5185\u5b58\u5206\u914d\u65b9\u6848\uff0c\u7cfb\u7edf\u7684\u8fd0\u884c\u5185\u5b58\u5206\u522b\u4e3a1GB\u30014GB\u30016GB\u3002</p> <pre><code>\u6ce8\u610f: chatdoc\u5fc5\u987b\u9009\u75284GB\u5185\u5b58\u7684\u7248\u672c\u3002\n</code></pre> <p>\u6587\u4ef6\u94fe\u63a5\uff1ahttps://pan.baidu.com/s/14asuFPzjpnhPOBExdO3XuA?pwd=c925</p> <ul> <li>\u4e0b\u8f7d\u6240\u9700\u7684\u5185\u5b58\u5206\u914d\u7b56\u7565\u5bf9\u5e94\u7684emmcboot.itb\u6587\u4ef6\u548cmulti.its\u6587\u4ef6\u6682\u653e\u5728\u7528\u6237\u76ee\u5f55\u4e0b\u3002</li> <li><code>cd ~</code></li> <li><code>sudo cp multi.its /boot</code></li> <li><code>sudo cp emmcboot.its /boot</code></li> <li><code>sudo reboot</code> \u91cd\u542fAirBox</li> </ul>"},{"location":"airbox/%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E4%BD%BF%E7%94%A8.html","title":"\u865a\u62df\u73af\u5883\u4f7f\u7528","text":"<p>airbox\u4e2d\u53ef\u4ee5\u4f7f\u7528\u591a\u79cd\u5e94\u7528\uff0c\u7136\u800c\u4e0d\u540c\u5e94\u7528\u4e4b\u95f4\u53ef\u80fd\u4f1a\u5b58\u5728\u7740\u4f9d\u8d56\u51b2\u7a81\uff0c\u4e3a\u4e86\u907f\u514d\u8fd9\u79cd\u60c5\u51b5\u7684\u53d1\u751f\uff0c\u6211\u4eec\u63a8\u8350\u4f7f\u7528\u865a\u62df\u73af\u5883\u6765\u6784\u5efa\u5e94\u7528\u7684\u8fd0\u884c\u73af\u5883\u3002\u4e0b\u9762\u5c06\u4ecb\u7ecd\u4f7f\u7528virtualenv\u6765\u7ba1\u7406\u865a\u62df\u73af\u5883\u3002</p>"},{"location":"airbox/%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E4%BD%BF%E7%94%A8.html#virtualenv","title":"\u5b89\u88c5virtualenv","text":"<ul> <li><code>pip3 install virtualenv -i https://pypi.tuna.tsinghua.edu.cn/simple</code></li> </ul>"},{"location":"airbox/%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E4%BD%BF%E7%94%A8.html#_1","title":"\u5e38\u7528\u64cd\u4f5c","text":"<ul> <li> <p>\u65b0\u5efa\u865a\u62df\u73af\u5883 <code>virtualenv env_name</code> </p> <p>\u5c06\u5728\u5f53\u524d\u76ee\u5f55\u4e0b\u65b0\u5efa\u4e00\u4e2a\u540d\u4e3aenv_name\u865a\u62df\u73af\u5883\uff0c\u865a\u62df\u73af\u5883\u7684\u6587\u4ef6\u5373\u5b58\u50a8\u5728\u5f53\u524d\u76ee\u5f55\u4e0b\u7684env_name\u6587\u4ef6\u5939\u91cc\u3002\u8fd9\u4e2a\u865a\u62df\u73af\u5883\u5305\u542b\u4e86\u5168\u5c40\u7684python\u548cpip\u7684\u62f7\u8d1d\u3002</p> </li> <li> <p>\u6fc0\u6d3b\u865a\u62df\u73af\u5883 <code>source env_name/bin/activate</code></p> <p>\u5c06\u6fc0\u6d3b\u5f53\u524d\u76ee\u5f55\u4e0b\u7684\u540d\u4e3aenv_name\u7684\u865a\u62df\u73af\u5883\uff0c\u5f53\u524d\u865a\u62df\u73af\u5883\u7684\u540d\u5b57\u4f1a\u663e\u793a\u5728\u63d0\u793a\u7b26\u5de6\u4fa7\uff0c\u4efb\u4f55\u60a8\u4f7f\u7528pip\u5b89\u88c5\u7684\u5305\u5c06\u4f1a\u653e\u5728env_name\u6587\u4ef6\u5939\u4e2d\uff0c\u4e0e\u5168\u5c40\u7684python\u9694\u7edd\u5f00\u3002</p> </li> <li> <p>\u9000\u51fa\u865a\u62df\u73af\u5883 <code>deactivate</code></p> <p>\u8fd9\u5c06\u4f1a\u56de\u5230\u7cfb\u7edf\u9ed8\u8ba4\u7684python\u89e3\u91ca\u5668\u548cpip\uff0c\u5305\u62ec\u5df2\u5b89\u88c5\u7684\u5e93\u4e5f\u4f1a\u56de\u5230\u9ed8\u8ba4\u7684\u3002</p> </li> <li> <p>\u5220\u9664\u865a\u62df\u73af\u5883 <code>rm -rf env_name</code></p> <p>\u5220\u9664\u865a\u62df\u73af\u5883\u5bf9\u5e94\u7684\u6587\u4ef6\u5939\u5373\u53ef\u3002</p> </li> </ul>"},{"location":"airbox/AI%E7%BB%98%E7%94%BB%E5%BA%94%E7%94%A8/SD%20v15%E5%AE%89%E8%A3%85%E5%90%AF%E5%8A%A8.html","title":"SD v15\u5b89\u88c5\u542f\u52a8","text":"<p>\u8fd9\u90e8\u5206\u5c06\u4ecb\u7ecd\u5982\u4f55\u542f\u52a8\u4e00\u4e2aStable Diffusion\u7a0b\u5e8f\u3002</p>"},{"location":"airbox/AI%E7%BB%98%E7%94%BB%E5%BA%94%E7%94%A8/SD%20v15%E5%AE%89%E8%A3%85%E5%90%AF%E5%8A%A8.html#_1","title":"\u6587\u4ef6\u4e0b\u8f7d","text":"<p>\u626b\u63cf\u4e0a\u9762\u8fd9\u4e2a\u4e8c\u7ef4\u7801\u4e0b\u8f7d\uff0c\u6216\u8005\u70b9\u51fb\u767e\u5ea6\u7f51\u76d8\u4e0b\u8f7d\u5730\u5740\u3002\u9879\u76ee\u8ddf\u76ee\u5f55\u662f<code>~/airbox-app/SDv15-airbox/</code>\u3002</p>"},{"location":"airbox/AI%E7%BB%98%E7%94%BB%E5%BA%94%E7%94%A8/SD%20v15%E5%AE%89%E8%A3%85%E5%90%AF%E5%8A%A8.html#_2","title":"\u9879\u76ee\u542f\u52a8","text":"<ul> <li>\u8fdb\u5165\u9879\u76ee\u8ddf\u76ee\u5f55\u3002</li> <li>\u5b89\u88c5\u4f9d\u8d56 <code>bash prepare.sh</code> </li> <li>\u542f\u52a8\u9879\u76ee <code>bash run.sh</code>\uff0c\u5982\u4e0a\u56fe\u542f\u52a8\u6210\u529f\u3002</li> </ul>"},{"location":"airbox/AI%E7%BB%98%E7%94%BB%E5%BA%94%E7%94%A8/SD%20v15%E5%AE%89%E8%A3%85%E5%90%AF%E5%8A%A8.html#_3","title":"\u9879\u76ee\u4e3b\u9875\u9762","text":""},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/ChatBot%E5%AE%89%E8%A3%85.html","title":"ChatBot\u5b89\u88c5","text":"<p>\u8fd9\u90e8\u5206\u5c06\u4ecb\u7ecd\u5982\u4f55\u542f\u52a8\u4e00\u4e2aChatBot\u7a0b\u5e8f\u3002</p>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/ChatBot%E5%AE%89%E8%A3%85.html#_1","title":"\u6587\u4ef6\u4e0b\u8f7d","text":"<p>\u626b\u63cf\u4e0a\u9762\u8fd9\u4e2a\u4e8c\u7ef4\u7801\u4e0b\u8f7d\uff0c\u6216\u8005\u70b9\u51fb\u767e\u5ea6\u7f51\u76d8\u4e0b\u8f7d\u5730\u5740\u3002\u53ef\u6839\u636e\u4f7f\u7528\u7684\u6587\u4ef6\uff0c\u6309\u9700\u4e0b\u8f7d\u3002</p>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/ChatBot%E5%AE%89%E8%A3%85.html#_2","title":"\u6a21\u578b\u6587\u4ef6","text":"<p>\u7f51\u76d8\u4e2d\u5171\u4e09\u79cd\u6a21\u578b\u6587\u4ef6\uff0c\u5206\u522b\u662fint8-2048\uff0cint8-1024\uff0cint4-512\uff0c\u4f4d\u4e8e<code>~/airbox-app/chatglm\u5e94\u7528/</code>\u8def\u5f84\u4e0b\u3002</p> <p>\u5047\u8bbe\u6211\u4eec\u4f7f\u7528int8-2048\u6a21\u578b(\u91c7\u7528int8\u7684\u91cf\u5316\uff0c\u6700\u5927token\u957f\u5ea6\u4e3a2048)\uff0c\u5177\u4f53\u64cd\u4f5c\u65b9\u6cd5\u662f<code>cp -r ~/airbox-app/chatglm\u5e94\u7528/chatglm-int8-2048 /data/</code>\uff0c\u5373\u62f7\u8d1dchatglm-int8-2048\u76ee\u5f55\u5230AirBox\u7684/data\u4e0b\u3002</p> <p>chatglm-int8-2048\u76ee\u5f55\u5305\u542b\u4e09\u4e2a\u6587\u4ef6\u4e00\u4e2a<code>chatglm2-6b_2048_int8.bmodel</code>\u6a21\u578b\u6587\u4ef6\uff0c\u4e00\u4e2a\u662f<code>libtpuchat.so</code>cpp\u7f16\u8bd1\u7684so\u6587\u4ef6\uff0c\u6700\u540e\u4e00\u4e2a\u662f<code>tokenizer.model</code>\u3002</p>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/ChatBot%E5%AE%89%E8%A3%85.html#chatdoc","title":"chatdoc\u9879\u76ee\u6587\u4ef6","text":"<ul> <li><code>cd /data/</code></li> <li><code>git clone https://github.com/zhengorange/chatbot.git</code></li> </ul>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/ChatBot%E5%AE%89%E8%A3%85.html#_3","title":"\u9879\u76ee\u7ed3\u6784\u6811","text":"<pre><code>|-- chatbot\n    |-- chat.py           -- Python\u8c03\u7528cpp\u63a8\u7406\u63a5\u53e3\u811a\u672c\n    |-- web_demo.py       -- \u9875\u9762\u4ea4\u4e92\u811a\u672c\n    |-- requirements.txt  -- \u9879\u76ee\u4f9d\u8d56\n    |-- config.ini        -- \u6a21\u578b\u6587\u4ef6\u914d\u7f6e\n</code></pre>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/ChatBot%E5%AE%89%E8%A3%85.html#configini","title":"config.ini\u914d\u7f6e\u6587\u4ef6","text":"<p><pre><code>[llm_model]\nlibtpuchat_path = ../chatglm-int8-2048/libtpuchat.so\nbmodel_path = ../chatglm-int8-2048/chatglm2-6b_2048_int8.bmodel\ntoken_path = ../chatglm-int8-2048/tokenizer.model\n</code></pre> config.ini\u9700\u8981\u914d\u7f6e\u6b63\u786e\u7684\u6a21\u578b\u6587\u4ef6\uff0c\u9ed8\u8ba4\u662f\u9009\u62e9int8-2048\u7684\u6a21\u578b\u3002\u82e5\u8981\u6539\u66f4\u4e3a\u5176\u4ed6\u6a21\u578b\u6587\u4ef6\uff0c\u8bf7\u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u6a21\u578b\u6587\u4ef6\u8def\u5f84\u3002</p>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/ChatBot%E5%AE%89%E8%A3%85.html#_4","title":"\u4f9d\u8d56\u5b89\u88c5","text":"<p>\u5728AirBox\u7ec8\u7aef\u8fdb\u5165\u5230/data/chatbot/\u76ee\u5f55\u4e0b\u3002\u6267\u884c</p> <p><code>source ../chatdoc/glm/bin/activate</code></p> <p><code>pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple</code></p> <p>\u5b89\u88c5\u9879\u76ee\u6240\u9700\u8981\u7684\u4f9d\u8d56\u3002</p>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/ChatBot%E5%AE%89%E8%A3%85.html#_5","title":"\u9879\u76ee\u542f\u52a8","text":"<ul> <li> <p>\u8fdb\u5165\u9879\u76ee\u76ee\u5f55<code>cd /data/chatbot</code></p> </li> <li> <p>\u82e5\u6ca1\u6709\u6fc0\u6d3bglm\u73af\u5883\uff0c\u6267\u884c<code>source ../chatdoc/glm/bin/activate</code>\uff0c\u82e5\u5df2\u6fc0\u6d3b\uff0c\u5219\u5ffd\u7565\u6b64\u6b65\u3002</p> </li> <li> <p>\u542f\u52a8\u9879\u76ee<code>python web_demo.py</code></p> </li> </ul>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/ChatBot%E6%93%8D%E4%BD%9C%E8%AF%B4%E6%98%8E.html","title":"ChatBot\u64cd\u4f5c\u8bf4\u660e","text":"<p>ChatBot\u4f9d\u6258ChatGLM2\uff0c\u53ef\u5b8c\u6210\u591a\u8f6e\u5bf9\u8bdd\u3002\u5728\u8f93\u5165\u6846\u8f93\u5165\u5b8c\u6210\u8981\u8be2\u95eeChatBot\u7684\u6d88\u606f\u540e\uff0c\u70b9\u51fb\u63d0\u4ea4\u540e\u7b49\u5f85\u6a21\u578b\u7684\u8fd4\u56de\u3002\u70b9\u51fbClear History\u6309\u94ae\u53ef\u6e05\u695a\u804a\u5929\u8bb0\u5f55\u3002</p>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/ChatDoc%20API%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E.html","title":"ChatDoc API\u4f7f\u7528\u8bf4\u660e","text":"<p>\u9664\u4e86\u901a\u8fc7WEB\u7684\u65b9\u6cd5\u4ea4\u4e92\uff0c\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u7b80\u5355\u7684API\u4ea4\u4e92\u65b9\u5f0f\u3002API\u670d\u52a1\u542f\u52a8\u65f6\u5c06\u81ea\u52a8\u52a0\u8f7d\u6301\u4e45\u5316\u7684\u7b2c\u4e00\u4e2a\u77e5\u8bc6\u5e93\uff0c\u5e76\u5bf9\u5916\u63d0\u4f9b\u4e24\u4e2a\u63a5\u53e3\uff0c\u53ef\u5b9e\u73b0\u7b80\u5355\u7684ChatDoc\u548cChatBot\u3002</p>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/ChatDoc%20API%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E.html#_1","title":"\u542f\u52a8\u65b9\u6cd5","text":"<p><code>bash api.sh</code></p>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/ChatDoc%20API%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E.html#chatdoc","title":"ChatDoc","text":"<ul> <li>\u63a5\u53e3\u540d\u79f0\uff1a<code>/chatdoc</code></li> <li>\u8bf7\u6c42\u53c2\u6570</li> </ul> \u53c2\u6570\u540d \u7c7b\u578b \u63cf\u8ff0 question str \u8981\u95ee\u7684\u95ee\u9898 <ul> <li>\u8fd4\u56de\u53c2\u6570</li> </ul> \u53c2\u6570\u540d \u7c7b\u578b \u63cf\u8ff0 result str \u6a21\u578b\u7684\u8f93\u51fa <ul> <li>\u8c03\u7528\u793a\u4f8b </li> </ul>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/ChatDoc%20API%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E.html#chatbot","title":"ChatBot","text":"<ul> <li>\u63a5\u53e3\u540d\u79f0\uff1a<code>/chatbot</code></li> <li>\u8bf7\u6c42\u53c2\u6570</li> </ul> \u53c2\u6570\u540d \u7c7b\u578b \u63cf\u8ff0 question str \u8981\u95ee\u7684\u95ee\u9898 history arr \u5386\u53f2\u804a\u5929\u8bb0\u5f55\u3002\u5e94\u8be5\u662f\u4e00\u4e2a\u4e8c\u7ef4\u6570\u7ec4\uff0c\u5982<code>[[\"\u4eba\u7c7b\u8bf4\", \"\u673a\u5668\u4eba\u8bf4\"], [\"\u4eba\u7c7b\u518d\u8bf4\", \"\u673a\u5668\u4eba\u518d\u8bf4\"]]</code>\u3002 <ul> <li>\u8fd4\u56de\u53c2\u6570</li> </ul> \u53c2\u6570\u540d \u7c7b\u578b \u63cf\u8ff0 result str \u6a21\u578b\u7684\u8f93\u51fa <ul> <li>\u8c03\u7528\u793a\u4f8b </li> </ul>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/ChatDoc%E5%AE%89%E8%A3%85.html","title":"ChatDoc\u5b89\u88c5","text":"<p>\u8fd9\u90e8\u5206\u5c06\u4ecb\u7ecd\u5982\u4f55\u542f\u52a8\u4e00\u4e2aChatDoc\u7a0b\u5e8f\u3002</p>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/ChatDoc%E5%AE%89%E8%A3%85.html#_1","title":"\u6587\u4ef6\u4e0b\u8f7d","text":"<p>\u626b\u63cf\u4e0a\u9762\u8fd9\u4e2a\u4e8c\u7ef4\u7801\u4e0b\u8f7d\uff0c\u6216\u8005\u70b9\u51fb\u767e\u5ea6\u7f51\u76d8\u4e0b\u8f7d\u5730\u5740\u3002\u53ef\u6839\u636e\u4f7f\u7528\u7684\u6587\u4ef6\uff0c\u6309\u9700\u4e0b\u8f7d\u3002</p>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/ChatDoc%E5%AE%89%E8%A3%85.html#_2","title":"\u6a21\u578b\u6587\u4ef6","text":"<p>chatdoc\u4ec5\u63a8\u8350\u4f7f\u7528int8-2048\u6a21\u578b\u3002\u56e0\u6b64\u6240\u9700\u6a21\u578b\u6587\u4ef6\u5728\u4f4d\u4e8e<code>~/airbox-app/chatglm\u5e94\u7528/chatglm-int8-2048/</code>\u8def\u5f84\u4e0b\u3002</p> <p>\u6267\u884c<code>cp -r ~/airbox-app/chatglm\u5e94\u7528/chatglm-int8-2048/ /data</code></p> <p>\u5c06chatglm-int8-2048\u76ee\u5f55\u62f7\u8d1d\u5230AirBox\u7684/data\u4e0b\u3002chatglm-int8-2048\u76ee\u5f55\u5305\u542b\u4e09\u4e2a\u6587\u4ef6\u4e00\u4e2a<code>chatglm2-6b_2048_int8.bmodel</code>\u6a21\u578b\u6587\u4ef6\uff0c\u4e00\u4e2a\u662f<code>libtpuchat.so</code>cpp\u7f16\u8bd1\u7684so\u6587\u4ef6\uff0c\u6700\u540e\u4e00\u4e2a\u662f<code>tokenizer.model</code>\u3002</p>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/ChatDoc%E5%AE%89%E8%A3%85.html#_3","title":"\u9879\u76ee\u7ed3\u6784\u6811","text":"<pre><code>|-- chatglm-int8-2048     -- \u6a21\u578b\u6587\u4ef6\n    |-- chatglm2-6b_2048_int8.bmodel\n    |-- libtpuchat.so\n    |-- tokenizer.model\n|-- chatdoc\n    |-- README.md         -- README\n    |-- api.py            -- API\u670d\u52a1\u811a\u672c\n    |-- chat.py           -- Python\u8c03\u7528cpp\u63a8\u7406\u63a5\u53e3\u811a\u672c\n    |-- chatbot.py        -- ChatDoc\u4e1a\u52a1\u903b\u8f91\u811a\u672c\n    |-- config.ini        -- \u63a8\u7406\u6a21\u578b\u914d\u7f6e\u6587\u4ef6\n    |-- requirements.txt  -- \u9879\u76ee\u4f9d\u8d56\n    |-- run.sh            -- \u542f\u52a8\u811a\u672c\n    |-- web_demo_st.py    -- \u9875\u9762\u4ea4\u4e92\u811a\u672c\n    |-- data          \n        |-- db            -- \u77e5\u8bc6\u5e93\u6301\u4e45\u5316\u76ee\u5f55\n        |-- uploaded      -- \u5df2\u4e0a\u4f20\u6587\u4ef6\u76ee\u5f55\n    |-- embedding_tpu     -- \u6587\u672c\u5d4c\u5165\u6a21\u578bTPU\u7248\u672c\n    |-- embedding         -- \u6587\u672c\u5d4c\u5165\u6a21\u578bCPU\u7248\u672c\n    |-- static            -- README\u4e2d\u56fe\u7247\u6587\u4ef6\n</code></pre>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/ChatDoc%E5%AE%89%E8%A3%85.html#chatdoc","title":"chatdoc\u9879\u76ee\u6587\u4ef6","text":"<ul> <li><code>cd /data/</code></li> <li><code>git clone https://github.com/zhengorange/chatdoc.git</code></li> </ul>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/ChatDoc%E5%AE%89%E8%A3%85.html#_4","title":"\u4f9d\u8d56\u5b89\u88c5","text":"<p>\u5728AirBox\u7ec8\u7aef\u8fdb\u5165\u5230/data/chatdoc/\u76ee\u5f55\u4e0b\u3002\u6267\u884c <code>virtualenv glm</code></p> <p><code>source glm/bin/activate</code></p> <p><code>pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple</code></p> <p><code>pip install tpu_perf-1.2.24-py3-none-manylinux2014_aarch64.whl</code></p> <p>\u5b89\u88c5\u9879\u76ee\u6240\u9700\u8981\u7684\u4f9d\u8d56\u3002</p> <p>\u6709\u4e9b\u7f51\u7edc\u6761\u4ef6\u4e0b\uff0cnltk\u8bed\u6599\u5c06\u4e0b\u8f7d\u5931\u8d25\uff0c\u89e3\u51b3\u529e\u6cd5\u662f\u5c06\u6211\u4eec\u63d0\u4f9b\u7684nltk_data\u6587\u4ef6\u5939\u62f7\u8d1d\u5230AirBox\u7528\u6237\u8ddf\u76ee\u5f55\uff0c\u5373<code>cp -r ~/airbox-app/nltk_data ~/</code>\u3002</p> <p>embedding\u5206\u4e3acpu\u7248\u672c\u548ctpu\u7248\u672c\uff0c\u4efb\u9009\u5176\u4e00\u5373\u53ef\u3002</p> <ul> <li> <p>TPU\u7248\u672c \u767e\u5ea6\u7f51\u76d8\u94fe\u63a5: https://pan.baidu.com/s/18wARtq7JdnzphUt9M9HScw?pwd=t2f3 \u4e0b\u8f7d\u5b8c\u6210\u5c06\u4e0b\u8f7d\u7684embedding_tpu\u6587\u4ef6\u5939\u66ff\u6362chatdoc\u76ee\u5f55\u4e0b\u7684embedding_tpu\u3002</p> </li> <li> <p>CPU\u7248\u672c \u767e\u5ea6\u7f51\u76d8\u94fe\u63a5: https://pan.baidu.com/s/1yFrk0Jtmbfr-nHnWvXF6AA?pwd=x5rw \u4e0b\u8f7d\u5b8c\u6210\u5c06\u4e0b\u8f7d\u7684embedding\u6587\u4ef6\u5939\u66ff\u6362chatdoc\u76ee\u5f55\u4e0b\u7684embedding\u3002</p> </li> </ul>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/ChatDoc%E5%AE%89%E8%A3%85.html#_5","title":"\u9879\u76ee\u542f\u52a8","text":"<ul> <li>\u8fdb\u5165\u9879\u76ee\u76ee\u5f55<code>cd /data/chatdoc</code></li> <li>\u542f\u52a8\u9879\u76ee<code>bash run.sh</code> \u6216 <code>bash run_emb_tpu.sh</code></li> </ul> <p>\u66f4\u591a\u9879\u76ee\u7ec6\u8282\u53c2\u8003GitHub\u4ed3\u5e93</p>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/ChatDoc%E6%93%8D%E4%BD%9C%E8%AF%B4%E6%98%8E.html","title":"ChatDoc\u64cd\u4f5c\u8bf4\u660e","text":""},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/ChatDoc%E6%93%8D%E4%BD%9C%E8%AF%B4%E6%98%8E.html#_1","title":"\u754c\u9762\u7b80\u4ecb","text":"<p>ChatDoc\u7531\u63a7\u5236\u533a\u548c\u804a\u5929\u5bf9\u8bdd\u533a\u7ec4\u6210\u3002\u63a7\u5236\u533a\u7528\u4e8e\u7ba1\u7406\u6587\u6863\u548c\u77e5\u8bc6\u5e93\uff0c\u804a\u5929\u5bf9\u8bdd\u533a\u7528\u4e8e\u8f93\u5165\u6d88\u606f\u63a5\u53d7\u6d88\u606f\u3002</p> <p>\u4e0a\u56fe\u4e2d\u768410\u53f7\u533a\u57df\u662fChatDoc\u5f53\u524d\u9009\u4e2d\u7684\u6587\u6863\uff0c\u82e510\u53f7\u533a\u57df\u4e3a\u7a7a\uff0c\u5373ChatDoc\u6ca1\u6709\u9009\u4e2d\u4efb\u4f55\u6587\u6863\uff0c\u800c\u6b64\u65f6\u4ecd\u5728\u804a\u5929\u5bf9\u8bdd\u533a\u4e0eChatDoc\u5bf9\u8bdd\uff0c\u53ea\u662f\u6b64\u65f6\u7684ChatDoc\u662f\u4e00\u4e2a\u5355\u7eaf\u4f9d\u6258ChatGLM2\u7684ChatBot\u3002</p>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/ChatDoc%E6%93%8D%E4%BD%9C%E8%AF%B4%E6%98%8E.html#_2","title":"\u4e0a\u4f20\u6587\u6863","text":"<p>\u70b9\u51fb<code>1</code>\u9009\u62e9\u8981\u4e0a\u4f20\u7684\u6587\u6863\uff0c\u7136\u540e\u70b9\u51fb\u6309\u94ae<code>4</code>\u6784\u5efa\u77e5\u8bc6\u5e93\u3002\u968f\u540e\u5c06embedding\u6587\u6863\uff0c\u5b8c\u6210\u540e\u5c06\u88ab\u9009\u4e2d\uff0c\u5e76\u663e\u793a\u572810\u53f7\u533a\u57df\uff0c\u63a5\u7740\u5c31\u53ef\u5f00\u59cb\u5bf9\u8bdd\u3002\u6211\u4eec\u53ef\u91cd\u590d\u4e0a\u4f20\u6587\u6863\uff0cembedding\u6210\u529f\u7684\u6587\u6863\u5747\u4f1a\u8fdb\u516510\u53f7\u533a\u57df\u3002</p>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/ChatDoc%E6%93%8D%E4%BD%9C%E8%AF%B4%E6%98%8E.html#_3","title":"\u6301\u4e45\u5316\u77e5\u8bc6\u5e93","text":"<p>10\u53f7\u533a\u57df\u9009\u4e2d\u7684\u6587\u6863\u5728\u7528\u6237\u5237\u65b0\u6216\u8005\u5173\u95ed\u9875\u9762\u65f6\uff0c\u5c06\u4f1a\u6e05\u7a7a\uff0c\u800c\u5982\u4f55\u80fd\u4fdd\u5b58\u8fd9\u4e9b\u5df2\u7ecfembedding\u7684\u6587\u6863\uff0c\u6211\u4eec\u53ef\u4ee5\u6301\u4e45\u5316\u77e5\u8bc6\u5e93\uff0c\u5728\u4e0b\u6b21\u8fdb\u5165\u65e0\u9700embedding\u8ba1\u7b97\u5373\u53ef\u52a0\u8f7d\u77e5\u8bc6\u5e93\u3002\u5177\u4f53\u505a\u6cd5\u662f\uff0c\u572810\u53f7\u533a\u57df\u4e0d\u4e3a\u7a7a\u7684\u60c5\u51b5\u4e0b\uff0c\u70b9\u51fb\u6309\u94ae<code>5</code>\u5373\u53ef\u6301\u4e45\u5316\u77e5\u8bc6\u5e93\uff0c\u77e5\u8bc6\u5e93\u7684\u540d\u79f0\u662f\u6240\u6709\u6587\u6863\u540d\u79f0\u4ee5\u9017\u53f7\u8fde\u63a5\u800c\u6210\u3002</p>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/ChatDoc%E6%93%8D%E4%BD%9C%E8%AF%B4%E6%98%8E.html#_4","title":"\u5bfc\u5165\u77e5\u8bc6\u5e93","text":"<p>\u8fdb\u5165ChatDoc\u6211\u4eec\u53ef\u4ee5\u4ece\u9009\u62e9\u6846<code>2</code>\u67e5\u770b\u76ee\u524d\u4ee5\u6301\u4e45\u5316\u7684\u77e5\u8bc6\u5e93\uff0c\u9009\u4e2d\u6211\u4eec\u9700\u8981\u52a0\u8f7d\u7684\u77e5\u8bc6\u5e93\u540e\uff0c\u70b9\u51fb\u6309\u94ae<code>3</code>\u5bfc\u5165\u77e5\u8bc6\u5e93\u3002\u5b8c\u6210\u540e\u5373\u53ef\u5f00\u59cb\u5bf9\u8bdd\u3002</p>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/ChatDoc%E6%93%8D%E4%BD%9C%E8%AF%B4%E6%98%8E.html#_5","title":"\u5220\u9664\u77e5\u8bc6\u5e93","text":"<p>\u5f53\u6211\u4eec\u9700\u8981\u5220\u9664\u672c\u5730\u5df2\u7ecf\u6301\u4e45\u5316\u7684\u77e5\u8bc6\u5e93\u65f6\uff0c\u6211\u4eec\u53ef\u4ece\u9009\u62e9\u6846<code>2</code>\u9009\u62e9\u6211\u4eec\u8981\u5220\u9664\u7684\u77e5\u8bc6\u5e93\uff0c\u7136\u540e\u70b9\u51fb\u6309\u94ae<code>6</code>\u5220\u9664\u77e5\u8bc6\u5e93\u3002</p>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/ChatDoc%E6%93%8D%E4%BD%9C%E8%AF%B4%E6%98%8E.html#_6","title":"\u91cd\u547d\u540d\u77e5\u8bc6\u5e93","text":"<p>\u7531\u4e8e\u77e5\u8bc6\u5e93\u7684\u547d\u540d\u662f\u7531\u5176\u6587\u6863\u7684\u540d\u79f0\u7ec4\u5408\u800c\u6765\uff0c\u96be\u514d\u9020\u6210\u77e5\u8bc6\u5e93\u540d\u79f0\u8fc7\u957f\u7684\u95ee\u9898\uff0cChatDoc\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4fee\u6539\u77e5\u8bc6\u5e93\u540d\u79f0\u7684\u529f\u80fd\uff0c\u9009\u62e9\u6846<code>2</code>\u9009\u62e9\u6211\u4eec\u8981\u4fee\u6539\u7684\u77e5\u8bc6\u5e93\uff0c\u7136\u540e\u70b9\u51fb\u6309\u94ae<code>9</code>\u91cd\u547d\u540d\u77e5\u8bc6\u5e93\uff0c\u968f\u540eChatDoc\u5c06\u5f39\u51fa\u4e00\u4e2a\u8f93\u5165\u6846\u548c\u4e00\u4e2a\u786e\u8ba4\u6309\u94ae\uff0c\u5982\u4e0a\u56fe\u3002\u5728\u8f93\u51fa\u6846\u8f93\u5165\u6211\u4eec\u4fee\u6539\u81f3\u7684\u540d\u79f0\uff0c\u7136\u540e\u70b9\u51fb\u786e\u8ba4\u91cd\u547d\u540d\u6309\u94ae\u3002</p>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/ChatDoc%E6%93%8D%E4%BD%9C%E8%AF%B4%E6%98%8E.html#_7","title":"\u6e05\u695a\u804a\u5929\u8bb0\u5f55","text":"<p>\u70b9\u51fb\u6309\u94ae<code>7</code>\u5373\u53ef\u6e05\u695a\u804a\u5929\u5bf9\u8bdd\u533a\u804a\u5929\u8bb0\u5f55\u3002\u5176\u4ed6\u4e0d\u53d7\u5f71\u54cd\u3002</p>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/ChatDoc%E6%93%8D%E4%BD%9C%E8%AF%B4%E6%98%8E.html#_8","title":"\u79fb\u9664\u9009\u4e2d\u6587\u6863","text":"<p>\u70b9\u51fb\u6309\u94ae<code>8</code>\u5c06\u6e05\u7a7a10\u53f7\u533a\u57df\uff0c\u540c\u65f6\u6e05\u695a\u804a\u5929\u8bb0\u5f55\u3002</p>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/%E6%95%99%E8%82%B2%E5%BA%94%E7%94%A8demo.html","title":"\u6559\u80b2\u5e94\u7528demo","text":"<p>\u8fd9\u90e8\u5206\u5c06\u4ecb\u7ecd\u5982\u4f55\u542f\u52a8\u4e00\u4e2aEduGPT\u7a0b\u5e8f\u3002</p>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/%E6%95%99%E8%82%B2%E5%BA%94%E7%94%A8demo.html#_1","title":"\u6587\u4ef6\u4e0b\u8f7d","text":"<p>\u626b\u63cf\u4e0a\u9762\u8fd9\u4e2a\u4e8c\u7ef4\u7801\u4e0b\u8f7d\uff0c\u6216\u8005\u70b9\u51fb\u767e\u5ea6\u7f51\u76d8\u4e0b\u8f7d\u5730\u5740\u3002\u53ef\u6839\u636e\u4f7f\u7528\u7684\u6587\u4ef6\uff0c\u6309\u9700\u4e0b\u8f7d\u3002</p>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/%E6%95%99%E8%82%B2%E5%BA%94%E7%94%A8demo.html#_2","title":"\u6a21\u578b\u6587\u4ef6","text":"<p>EduGPT\u4ec5\u63a8\u8350\u4f7f\u7528int8-2048\u6a21\u578b\u3002\u56e0\u6b64\u6240\u9700\u6a21\u578b\u6587\u4ef6\u5728\u4f4d\u4e8e<code>~/airbox-app/chatglm\u5e94\u7528/chatglm-int8-2048/</code>\u8def\u5f84\u4e0b\u3002</p> <p>\u6267\u884c<code>cp -r ~/airbox-app/chatglm\u5e94\u7528/chatglm-int8-2048/ /data</code></p> <p>\u5c06chatglm-int8-2048\u76ee\u5f55\u62f7\u8d1d\u5230AirBox\u7684/data\u4e0b\u3002chatglm-int8-2048\u76ee\u5f55\u5305\u542b\u4e09\u4e2a\u6587\u4ef6\u4e00\u4e2a<code>chatglm2-6b_2048_int8.bmodel</code>\u6a21\u578b\u6587\u4ef6\uff0c\u4e00\u4e2a\u662f<code>libtpuchat.so</code>cpp\u7f16\u8bd1\u7684so\u6587\u4ef6\uff0c\u6700\u540e\u4e00\u4e2a\u662f<code>tokenizer.model</code>\u3002</p>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/%E6%95%99%E8%82%B2%E5%BA%94%E7%94%A8demo.html#_3","title":"\u9879\u76ee\u7ed3\u6784\u6811","text":"<pre><code>|-- chatglm-int8-2048     -- \u6a21\u578b\u6587\u4ef6\n    |-- chatglm2-6b_2048_int8.bmodel\n    |-- libtpuchat.so\n    |-- tokenizer.model\n|-- airbox_edugpt\n    |-- ...\n    |-- app.py            -- \u542f\u52a8\u811a\u672c\n</code></pre>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/%E6%95%99%E8%82%B2%E5%BA%94%E7%94%A8demo.html#chatdoc","title":"chatdoc\u9879\u76ee\u6587\u4ef6","text":"<ul> <li><code>cd /data/</code></li> <li><code>git clone https://github.com/zhengorange/airbox_edugpt.git</code></li> </ul>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/%E6%95%99%E8%82%B2%E5%BA%94%E7%94%A8demo.html#_4","title":"\u4f9d\u8d56\u5b89\u88c5","text":"<p>\u5728AirBox\u7ec8\u7aef\u8fdb\u5165\u5230/data/airbox_edugpt/\u76ee\u5f55\u4e0b\u3002\u6267\u884c</p> <p><code>pip3 install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple</code></p> <p>\u5b89\u88c5\u9879\u76ee\u6240\u9700\u8981\u7684\u4f9d\u8d56\u3002</p> <p>\u6709\u4e9b\u7f51\u7edc\u6761\u4ef6\u4e0b\uff0cnltk\u8bed\u6599\u5c06\u4e0b\u8f7d\u5931\u8d25\uff0c\u89e3\u51b3\u529e\u6cd5\u662f\u5c06\u6211\u4eec\u63d0\u4f9b\u7684nltk_data\u6587\u4ef6\u5939\u62f7\u8d1d\u5230AirBox\u7528\u6237\u8ddf\u76ee\u5f55\uff0c\u5373<code>cp -r ~/airbox-app/nltk_data ~/</code>\u3002</p> <p>\u767e\u5ea6\u7f51\u76d8\u94fe\u63a5: https://pan.baidu.com/s/1yFrk0Jtmbfr-nHnWvXF6AA?pwd=x5rw \u4e0b\u8f7d\u5b8c\u6210\u5c06\u4e0b\u8f7d\u7684embedding\u6587\u4ef6\u5939\u66ff\u6362airbox_edugpt\u76ee\u5f55\u4e0b\u7684embedding\u3002</p>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/%E6%95%99%E8%82%B2%E5%BA%94%E7%94%A8demo.html#_5","title":"\u9879\u76ee\u542f\u52a8","text":"<ul> <li>\u8fdb\u5165\u9879\u76ee\u76ee\u5f55<code>cd /data/airbox_edugpt</code></li> <li>\u542f\u52a8\u9879\u76ee<code>python3 app.py</code></li> </ul>"},{"location":"airbox/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/%E6%A8%A1%E5%9E%8B%E8%B0%83%E7%94%A8%E8%AF%A6%E6%83%85.html","title":"\u6a21\u578b\u8c03\u7528\u8be6\u60c5","text":"<p>ChatDoc\u548cChatBot\u90fd\u662f\u57fa\u4e8eChatGLM2-TPU\u6784\u5efa\u7684\uff0c\u9879\u76ee\u53ef\u5728BM1684X\u4e0a\u72ec\u7acb\u90e8\u7f72\u8fd0\u884c\u3002\u5e94\u7528\u548c\u63a8\u7406\u662f\u89e3\u8026\u3002</p> <p></p> <p>\u7b80\u5355\u6765\u8bb2\uff0c\u6211\u4eec\u7684\u5e94\u7528\u90fd\u662f\u4f7f\u7528\u4e00\u4e2apython\u7684\u6d41\u5f0f\u63a8\u7406\u65b9\u6cd5<code>stream_predict(self, query, history)</code>\uff0c\u6211\u4eec\u4ec5\u9700\u8981\u4f20\u5165\u4e00\u4e2a\u6700\u65b0\u95ee\u9898\u548c\u4e00\u4e2a\u5386\u53f2\u804a\u5929\u8bb0\u5f55\uff0c\u51fd\u6570\u8fd4\u56de\u4e00\u4e2a\u751f\u6210\u5668\uff0c\u901a\u8fc7\u8fed\u4ee3\u6765\u83b7\u53d6\u6a21\u578b\u7684\u6bcf\u4e00\u6b65\u8f93\u51fa\u3002</p>"},{"location":"airbox/%E6%99%BA%E8%83%BD%E8%AF%AD%E9%9F%B3%E5%BA%94%E7%94%A8/%E8%AF%AD%E9%9F%B3%E6%91%98%E8%A6%81.html","title":"\u8bed\u97f3\u6458\u8981","text":"<p>\u8bed\u97f3\u6458\u8981\u7cfb\u7edf\u5305\u62ec\u4e24\u4e2a\u90e8\u5206\u7ec4\u6210\uff0c\u7b2c\u4e00\u4e2a\u90e8\u5206\u662fwhisper\u5f15\u64ce\uff0c\u7b2c\u4e8c\u4e2a\u90e8\u5206\u662fwebui\u3002\u4e0b\u9762\u4ecb\u7ecd\u5982\u4f55\u5b89\u88c5\u8bed\u97f3\u6458\u8981\u7cfb\u7edf\u3002</p>"},{"location":"airbox/%E6%99%BA%E8%83%BD%E8%AF%AD%E9%9F%B3%E5%BA%94%E7%94%A8/%E8%AF%AD%E9%9F%B3%E6%91%98%E8%A6%81.html#_1","title":"\u4e0b\u8f7d\u9879\u76ee","text":"<ol> <li><code>cd /data</code> \u8fdb\u5165data\u76ee\u5f55\uff0c\u5728\u6b64\u76ee\u5f55\u4e0b\u5b89\u88c5whisper\u5f15\u64ce\u3002</li> <li><code>git clone -b release https://github.com/JKay0327/whisper-TPU_py</code> \u62c9\u53d6\u6700\u65b0\u7684whisper\u5f15\u64ce\u3002</li> <li>\u4ece\u767e\u5ea6\u7f51\u76d8\u4e0b\u8f7dwhisper\u6240\u9700\u7684bmodel\u6587\u4ef6 <code>https://pan.baidu.com/s/1IakvYn9dWkg3mmxEo9medA?pwd=k4ih</code>\uff0c\u7136\u540e\u5c06bmodel\u6587\u4ef6\u5939\u79fb\u52a8\u5230<code>/data/whisper-TPU_py/</code>\u4e0b\u3002</li> <li>\u4ece\u767e\u5ea6\u7f51\u76d8\u4e0b\u8f7dchatglm2\u6240\u9700\u7684bmodel\u6587\u4ef6<code>https://pan.baidu.com/s/1i5r1RLnWRXbSL1KbDnqq-A?pwd=nffa</code>\uff0c\u7136\u540e\u5c06chatglm-int8-1024\u6587\u4ef6\u5939\u62f7\u8d1d\u5230<code>/data/</code>\u4e0b\u3002</li> <li><code>cd /data</code> \u8fdb\u5165data\u76ee\u5f55\uff0c\u5728\u6b64\u76ee\u5f55\u4e0b\u5b89\u88c5webui\u3002</li> <li><code>git clone https://github.com/zhengorange/Whisper-WebUI.git</code> \u62c9\u53d6\u6700\u65b0\u7684webui\u3002</li> </ol>"},{"location":"airbox/%E6%99%BA%E8%83%BD%E8%AF%AD%E9%9F%B3%E5%BA%94%E7%94%A8/%E8%AF%AD%E9%9F%B3%E6%91%98%E8%A6%81.html#_2","title":"\u65b0\u5efa\u865a\u62df\u73af\u5883","text":"<ol> <li><code>cd /data/whisper-TPU_py</code> \u8fdb\u5165\u9879\u76ee\u6587\u4ef6\u5939\u3002</li> <li>\u82e5\u5f53\u524dpip\u73af\u5883\u6ca1\u6709virtualenv\uff0c\u8bf7\u5148\u5b89\u88c5virtualenv\uff0c\u6267\u884c<code>pip3 install virtualenv -i https://pypi.tuna.tsinghua.edu.cn/simple</code>\uff0c\u82e5\u5f53\u524d\u73af\u5883\u5df2\u6709virtualenv\uff0c\u8df3\u8fc7\u6b64\u6b65\u9aa4\u3002</li> <li><code>virtualenv whisper</code> \u65b0\u5efa\u4e00\u4e2a\u540d\u53ebwhisper\u7684\u865a\u62df\u73af\u5883\u3002</li> <li><code>source whisper/bin/activate</code> \u6fc0\u6d3bwhisper\u73af\u5883\u3002</li> </ol>"},{"location":"airbox/%E6%99%BA%E8%83%BD%E8%AF%AD%E9%9F%B3%E5%BA%94%E7%94%A8/%E8%AF%AD%E9%9F%B3%E6%91%98%E8%A6%81.html#_3","title":"\u5b89\u88c5\u4f9d\u8d56","text":"<ol> <li><code>cd /data/whisper-TPU_py</code> \u8fdb\u5165\u9879\u76ee\u6587\u4ef6\u5939\u3002</li> <li><code>pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple</code></li> <li><code>python setup.py install</code></li> <li><code>export LOG_LEVEL=-1</code></li> <li><code>cd /data/Whisper-WebUI</code> \u8fdb\u5165\u9879\u76ee\u6587\u4ef6\u5939\u3002</li> <li><code>pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple</code></li> </ol>"},{"location":"airbox/%E6%99%BA%E8%83%BD%E8%AF%AD%E9%9F%B3%E5%BA%94%E7%94%A8/%E8%AF%AD%E9%9F%B3%E6%91%98%E8%A6%81.html#_4","title":"\u542f\u52a8\u9879\u76ee","text":"<p>\u8bf7\u68c0\u67e5\u5f53\u524d\u7ec8\u7aef\u662f\u5426\u6fc0\u6d3bwhisper\u865a\u62df\u73af\u5883\uff0c\u82e5\u6ca1\u6709\uff0c\u8bf7\u6267\u884c <code>source ../whisper-TPU_py/whisper/bin/activate</code> \u6fc0\u6d3bwhipser\u73af\u5883\u3002</p> <p>\u6267\u884c <code>python main.py</code> \u542f\u52a8\u9879\u76ee\u3002</p>"},{"location":"airbox/%E6%99%BA%E8%83%BD%E8%AF%AD%E9%9F%B3%E5%BA%94%E7%94%A8/%E8%AF%AD%E9%9F%B3%E6%91%98%E8%A6%81.html#_5","title":"\u9000\u51fa\u5f53\u524d\u73af\u5883","text":"<p><code>deactivate</code></p>"}]}